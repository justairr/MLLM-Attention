{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP not installed\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llava.eval.run_llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaLlamaForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_name_from_path\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_llava\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eval_model\n\u001b[1;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliuhaotian/llava-v1.5-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m tokenizer, model, image_processor, context_len \u001b[38;5;241m=\u001b[39m load_pretrained_model(\n\u001b[1;32m      9\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m     10\u001b[0m     model_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mget_model_name_from_path(model_path)\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llava.eval.run_llava'"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llava v1.5 patch\n",
    "def patch_prepare(\n",
    "        self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "        images, image_sizes=None\n",
    "    ):\n",
    "        vision_tower = self.get_vision_tower()\n",
    "        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n",
    "\n",
    "        if type(images) is list or images.ndim == 5:\n",
    "            if type(images) is list:\n",
    "                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n",
    "            concat_images = torch.cat([image for image in images], dim=0)\n",
    "            image_features = self.encode_images(concat_images)\n",
    "            split_sizes = [image.shape[0] for image in images]\n",
    "            image_features = torch.split(image_features, split_sizes, dim=0)\n",
    "            mm_patch_merge_type = getattr(self.config, 'mm_patch_merge_type', 'flat')\n",
    "            image_aspect_ratio = getattr(self.config, 'image_aspect_ratio', 'square')\n",
    "            if mm_patch_merge_type == 'flat':\n",
    "                image_features = [x.flatten(0, 1) for x in image_features]\n",
    "            elif mm_patch_merge_type.startswith('spatial'):\n",
    "                new_image_features = []\n",
    "                for image_idx, image_feature in enumerate(image_features):\n",
    "                    if image_feature.shape[0] > 1:\n",
    "                        base_image_feature = image_feature[0]\n",
    "                        image_feature = image_feature[1:]\n",
    "                        height = width = self.get_vision_tower().num_patches_per_side\n",
    "                        assert height * width == base_image_feature.shape[0]\n",
    "                        if image_aspect_ratio == 'anyres':\n",
    "                            num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, self.get_vision_tower().config.image_size)\n",
    "                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                        if 'unpad' in mm_patch_merge_type:\n",
    "                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n",
    "                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n",
    "                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n",
    "                            image_feature = torch.cat((\n",
    "                                image_feature,\n",
    "                                self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)\n",
    "                            ), dim=-1)\n",
    "                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n",
    "                        else:\n",
    "                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n",
    "                            image_feature = image_feature.flatten(0, 3)\n",
    "                        image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n",
    "                    else:\n",
    "                        image_feature = image_feature[0]\n",
    "                        if 'unpad' in mm_patch_merge_type:\n",
    "                            image_feature = torch.cat((\n",
    "                                image_feature,\n",
    "                                self.model.image_newline[None].to(image_feature.device)\n",
    "                            ), dim=0)\n",
    "                    new_image_features.append(image_feature)\n",
    "                image_features = new_image_features\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n",
    "        else:\n",
    "            image_features = self.encode_images(images)\n",
    "\n",
    "        if self.embed_weight is not None:\n",
    "            print(image_features.shape)\n",
    "            print(self.embed_weight.shape)\n",
    "            print(image_features)\n",
    "            print(self.embed_weight)\n",
    "            image_features *= self.embed_weight[:, None].to(image_features.device)\n",
    "\n",
    "        # TODO: image start / end is not implemented here to support pretraining.\n",
    "        if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Let's just add dummy tensors if they do not exist,\n",
    "        # it is a headache to deal with None all the time.\n",
    "        # But it is not ideal, and if you have a better idea,\n",
    "        # please open an issue / submit a PR, thanks.\n",
    "        _labels = labels\n",
    "        _position_ids = position_ids\n",
    "        _attention_mask = attention_mask\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "        else:\n",
    "            attention_mask = attention_mask.bool()\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "        if labels is None:\n",
    "            labels = torch.full_like(input_ids, IGNORE_INDEX)\n",
    "\n",
    "        # remove the padding using attention_mask -- FIXME\n",
    "        _input_ids = input_ids\n",
    "        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = []\n",
    "        cur_image_idx = 0\n",
    "        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "            if num_images == 0:\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
    "                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "                new_input_embeds.append(cur_input_embeds)\n",
    "                new_labels.append(labels[batch_idx])\n",
    "                cur_image_idx += 1\n",
    "                continue\n",
    "\n",
    "            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
    "            cur_input_ids_noim = []\n",
    "            cur_labels = labels[batch_idx]\n",
    "            cur_labels_noim = []\n",
    "            for i in range(len(image_token_indices) - 1):\n",
    "                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "                cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "            cur_new_input_embeds = []\n",
    "            cur_new_labels = []\n",
    "\n",
    "            for i in range(num_images + 1):\n",
    "                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "                cur_new_labels.append(cur_labels_noim[i])\n",
    "                if i < num_images:\n",
    "                    cur_image_features = image_features[cur_image_idx]\n",
    "                    cur_image_idx += 1\n",
    "                    cur_new_input_embeds.append(cur_image_features)\n",
    "                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "\n",
    "            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "            cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "            new_input_embeds.append(cur_new_input_embeds)\n",
    "            new_labels.append(cur_new_labels)\n",
    "\n",
    "        # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "        if tokenizer_model_max_length is not None:\n",
    "            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "\n",
    "        # Combine them\n",
    "        max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "        batch_size = len(new_input_embeds)\n",
    "\n",
    "        new_input_embeds_padded = []\n",
    "        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "            cur_len = cur_new_embed.shape[0]\n",
    "            if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),\n",
    "                    cur_new_embed\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, -cur_len:] = cur_new_labels\n",
    "                    attention_mask[i, -cur_len:] = True\n",
    "                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "            else:\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    cur_new_embed,\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, :cur_len] = cur_new_labels\n",
    "                    attention_mask[i, :cur_len] = True\n",
    "                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "\n",
    "        if _labels is None:\n",
    "            new_labels = None\n",
    "        else:\n",
    "            new_labels = new_labels_padded\n",
    "\n",
    "        if _attention_mask is None:\n",
    "            attention_mask = None\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "\n",
    "        if _position_ids is None:\n",
    "            position_ids = None\n",
    "\n",
    "        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n",
    "\n",
    "model.prepare_inputs_labels_for_multimodal = patch_prepare.__get__(model, LlavaLlamaForCausalLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "conv = conv_templates[\"llava_v1\"].copy()\n",
    "\n",
    "image_path_or_url = \"examples/image.png\" # , \"examples/sample.png\"]\n",
    "prompt_text = \"Describe the images.\"\n",
    "\n",
    "image_data = load_image(image_path_or_url)\n",
    "\n",
    "image_tensor = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "image_size = image_data.size\n",
    "if type(image_tensor) is list:\n",
    "    print(\"1\")\n",
    "    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n",
    "else:\n",
    "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
    "\n",
    "if model.config.mm_use_im_start_end:\n",
    "    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + prompt_text\n",
    "else:\n",
    "    inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt_text\n",
    "\n",
    "\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed_weight = torch.full((576,), 0.5)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=[image_size],\n",
    "        do_sample=False,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "\n",
    "text = tokenizer.decode(outputs[\"sequences\"][0]).strip()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "def get_mean_attn_score(output_ids) -> Tuple[torch.Tensor, int, int]:\n",
    "    r\"\"\"\n",
    "    get the mean attention weights of the prefilling and full attention\n",
    "    Args:\n",
    "        output_ids: the output ids of the model\n",
    "    Returns:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "    \"\"\"\n",
    "    output_attn = output_ids.attentions\n",
    "    pref_len = output_attn[0][0].shape[3]\n",
    "    full_len = output_attn[-1][0].shape[3]\n",
    "    prefill_attn = output_attn[0]\n",
    "    assert prefill_attn[0].shape[0] == 1, \"batch size should be 1\"\n",
    "    full_attn = []\n",
    "\n",
    "    for l, layer in enumerate(prefill_attn):\n",
    "        layer = layer.cpu().squeeze(0).float()\n",
    "        layer = torch.nn.functional.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "        for i in range(full_len - pref_len):\n",
    "            cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "            layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "        full_attn.append(layer)\n",
    "    mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "    return mean_attn, pref_len, full_len\n",
    "\n",
    "\n",
    "aw, pref_len, full_len = get_mean_attn_score(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aw.shape, pref_len, full_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def get_visual_token_mean_attn_score_llava(mean_attn, input_ids, pref_len, visual_token_id=-200, image_token_count=576) -> List[torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Get the attention weights of the visual tokens\n",
    "    Args:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "        inputs: the inputs of the model\n",
    "    Returns:\n",
    "        visual_token_attn_weights: the tuple of the attention weights of the visual tokens, each element shape: (V, V)\n",
    "    \"\"\"\n",
    "    assert input_ids.shape[0] == 1, \"batch size should be 1\"\n",
    "    vision_start_token_indices = torch.where(\n",
    "        input_ids[0] == visual_token_id\n",
    "    )[0]\n",
    "    visual_token_attn_weights = []\n",
    "    for i, s in enumerate(vision_start_token_indices):\n",
    "        print(s.item(), (s + i*image_token_count).item(), (s + (i+1)*image_token_count).item())\n",
    "        visual_token_attn_weights.append(\n",
    "            torch.mean(mean_attn[pref_len:, s + i*image_token_count : s + (i+1)*image_token_count], dim=0)\n",
    "        )\n",
    "    return visual_token_attn_weights\n",
    "\n",
    "\n",
    "vw = get_visual_token_mean_attn_score_llava(aw, input_ids, pref_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
