{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"MAX_PIXELS\"]=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9c153550bf41d592fe2c1a8c0ece37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlavaForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "model_id= \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    " model_id,\n",
    " torch_dtype=torch.bfloat16,\n",
    " attn_implementation=\"eager\",\n",
    "#  device_map=\"auto\",\n",
    ").to(\"cuda\")\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/image.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Describe this image.\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "#print(image_inputs)\n",
    "image = Image.open(\"examples/image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_images(image_list, mode=\"noise\", color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    根据 `mode` 生成与 `image_list` 中图像等大的随机噪声或纯色图像。\n",
    "\n",
    "    参数：\n",
    "    - image_list: list[PIL.Image]，输入的图像列表。\n",
    "    - mode: str，\"noise\" 生成随机噪声图像，\"blank\" 生成纯色图像。\n",
    "    - color: tuple，生成纯色图像时的颜色，默认为白色 (255, 255, 255)。\n",
    "\n",
    "    返回：\n",
    "    - list[PIL.Image]，生成的图像列表。\n",
    "    \"\"\"\n",
    "    generated_images = []\n",
    "    \n",
    "    for img in image_list:\n",
    "        width, height = img.size\n",
    "        \n",
    "        if mode == \"noise\":\n",
    "            noise_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n",
    "            generated_image = Image.fromarray(noise_array)\n",
    "        elif mode == \"blank\":\n",
    "            generated_image = Image.new(\"RGB\", (width, height), color)\n",
    "        else:\n",
    "            raise ValueError(\"mode could only be 'noise' or 'blank'\")\n",
    "        \n",
    "        generated_images.append(generated_image)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "# neg_image_inputs = generate_images(image_inputs, mode=\"noise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    images=[image],\n",
    "    text=[text],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "output_ids = model.generate(\n",
    "            **inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg_image_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m neg_inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39m[text],\n\u001b[0;32m----> 3\u001b[0m     images\u001b[38;5;241m=\u001b[39m\u001b[43mneg_image_inputs\u001b[49m,\n\u001b[1;32m      4\u001b[0m     videos\u001b[38;5;241m=\u001b[39mvideo_inputs,\n\u001b[1;32m      5\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m neg_inputs \u001b[38;5;241m=\u001b[39m neg_inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(neg_inputs\u001b[38;5;241m.\u001b[39mpixel_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neg_image_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "neg_inputs = processor(\n",
    "    text=[text],\n",
    "    images=neg_image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "neg_inputs = neg_inputs.to(\"cuda\")\n",
    "print(neg_inputs.pixel_values)\n",
    "negative_output_ids = model.generate(\n",
    "            **neg_inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image displays two different amounts of money, with one being $1,145,000 and the other being $4,999,999. The two amounts are placed side by side, with the larger amount on the left and the smaller amount on the right. The contrast between the two figures emphasizes the significant difference in their values.\n"
     ]
    }
   ],
   "source": [
    "# If you wanna check what's the output of negative generation, run me.\n",
    "\n",
    "# generated_ids = negative_output_ids.sequences\n",
    "generated_ids = output_ids.sequences\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def get_mean_attn_score(output_ids) -> Tensor:\n",
    "    r\"\"\"\n",
    "    get the mean attention weights of the prefilling and full attention\n",
    "    Args:\n",
    "        output_ids: the output ids of the model\n",
    "    Returns:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "    \"\"\"\n",
    "    output_attn = output_ids.attentions\n",
    "    pref_len = output_attn[0][0].shape[3]\n",
    "    full_len = output_attn[-1][0].shape[3]\n",
    "    prefill_attn = output_attn[0]\n",
    "    assert prefill_attn[0].shape[0] == 1, \"batch size should be 1\"\n",
    "    full_attn = []\n",
    "\n",
    "    for l, layer in enumerate(prefill_attn):\n",
    "        layer = layer.cpu().squeeze(0).float()\n",
    "        layer = torch.nn.functional.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "        for i in range(full_len - pref_len):\n",
    "            cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "            layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "        full_attn.append(layer)\n",
    "    mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "    return mean_attn\n",
    "\n",
    "aw = get_mean_attn_score(output_ids)\n",
    "\n",
    "# neg_aw = get_mean_attn_score(negative_output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[581]\n"
     ]
    }
   ],
   "source": [
    "from torch._tensor import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_visual_token_mean_attn_score(\n",
    "    mean_attn, inputs, vision_token_id\n",
    ") -> Tuple[Tensor, ...]:\n",
    "    r\"\"\"\n",
    "    Get the attention weights of the visual tokens\n",
    "    Args:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "        inputs: the inputs of the model\n",
    "    Returns:\n",
    "        visual_token_attn_weights: the tuple of the attention weights of the visual tokens, each element shape: (V, V)\n",
    "    \"\"\"\n",
    "    NUM_IMG_TOKENS = 576\n",
    "    assert inputs[\"input_ids\"].shape[0] == 1, \"batch size should be 1\"\n",
    "    pref_len = len(inputs[\"input_ids\"][0])\n",
    "    vision_start_token_indices = inputs[\"input_ids\"][0].tolist().index(vision_token_id)\n",
    "    vision_end_token_indices = vision_start_token_indices + NUM_IMG_TOKENS\n",
    "    vision_start_token_indices = [vision_start_token_indices]\n",
    "    vision_end_token_indices = [vision_end_token_indices]\n",
    "    # assert len(vision_start_token_indices) == len(vision_end_token_indices), \"vision start and end token idx should be the same\"\n",
    "    print(vision_start_token_indices)\n",
    "    print(vision_end_token_indices)\n",
    "    # iterate over multiple images\n",
    "    visual_token_attn_weights = tuple(\n",
    "        torch.mean(mean_attn[pref_len:, s : e], dim=0)\n",
    "        for s, e in zip(\n",
    "            vision_start_token_indices, vision_end_token_indices, strict=True\n",
    "        )\n",
    "    )\n",
    "    return visual_token_attn_weights\n",
    "\n",
    "\n",
    "vw = get_visual_token_mean_attn_score(\n",
    "    aw, inputs, processor.tokenizer.added_tokens_encoder[\"<image>\"]\n",
    ")\n",
    "# neg_vw = get_visual_token_mean_attn_score(neg_aw, neg_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([576])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "# Apply weighted attention to vision tokens, fix multiple images\n",
    "def get_visual_token_weight(\n",
    "    vision_attn_weight,\n",
    "    keep_percentage,\n",
    "    weighting_type: Literal[\"linear\", \"uniform\"] = \"linear\",\n",
    "    lowest_weight=0.0,\n",
    "):\n",
    "    sorted_indices = torch.argsort(vision_attn_weight, descending=True)\n",
    "    num_tokens_to_keep = int(len(vision_attn_weight) * keep_percentage)\n",
    "    weight_vision_token = torch.zeros_like(vision_attn_weight, dtype=torch.float)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    if weighting_type == \"linear\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(\n",
    "            lowest_weight, 1.0, len(vision_attn_weight) - num_tokens_to_keep\n",
    "        )\n",
    "    else:\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = lowest_weight\n",
    "    return weight_vision_token\n",
    "\n",
    "vm_linear = [get_visual_token_weight(v, 0.6, \"linear\", 0.0) for v in vw]\n",
    "vm_uniform = [get_visual_token_weight(v, 0.6, \"uniform\", 0.0) for v in vw]\n",
    "\n",
    "# model.embed_weight = torch.concat(vm_linear, dim=0).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 593])\n",
      "torch.Size([1, 3, 336, 336])\n",
      "torch.Size([576])\n",
      "torch.Size([1, 593, 4096])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Manually process input_ids and pixel_values into input_embeds\n",
    "def manual_embed_inputs(model, input_ids, pixel_values, vision_token_weights=None):\n",
    "    print(input_ids.shape)\n",
    "    print(pixel_values.shape)\n",
    "    print(vision_token_weights.shape)\n",
    "    \n",
    "    # Get input embeddings from the model\n",
    "    inputs_embeds = model.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    # Process image features\n",
    "    image_features = model.get_image_features(\n",
    "        pixel_values=pixel_values,\n",
    "        vision_feature_layer=model.config.vision_feature_layer,\n",
    "        vision_feature_select_strategy=model.config.vision_feature_select_strategy\n",
    "    )\n",
    "    \n",
    "    # Apply vision token weights if provided\n",
    "    if vision_token_weights is not None:\n",
    "        # Ensure weights are on the same device as image features\n",
    "        vision_token_weights = vision_token_weights.to(image_features.device)\n",
    "        # Apply weights to image features\n",
    "        image_features = image_features * vision_token_weights.unsqueeze(0).unsqueeze(-1)\n",
    "    \n",
    "    # Find image token positions\n",
    "    special_image_mask = (input_ids == model.config.image_token_index).unsqueeze(-1)\n",
    "    special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n",
    "    print(special_image_mask.shape)\n",
    "    \n",
    "    # Replace image tokens with image features\n",
    "    image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n",
    "    \n",
    "    return inputs_embeds\n",
    "\n",
    "# Create input embeddings with weighted vision tokens\n",
    "input_embeds = manual_embed_inputs(\n",
    "    model, \n",
    "    inputs.input_ids, \n",
    "    inputs.pixel_values, \n",
    "    vision_token_weights=vm_linear[0] if len(vm_linear) > 0 else None\n",
    ")\n",
    "\n",
    "# Create modified inputs dictionary\n",
    "modified_inputs = {\n",
    "    \"inputs_embeds\": input_embeds,\n",
    "    \"attention_mask\": inputs.attention_mask\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 593, 4096])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_inputs['inputs_embeds'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**modified_inputs, max_new_tokens=128)\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image displays two columns of numbers, likely representing financial data. The first column contains the numbers 1 through 10, while the second column has the numbers 11 through 15. The numbers in the second column are larger than those in the first column.\n",
      "\n",
      "In addition to the numbers, there is a question mark located in the middle of the second column, possibly indicating a point of interest or a question related to the financial data.\n"
     ]
    }
   ],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and echo output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids.sequences)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from torch import Tensor\n",
    "import copy\n",
    "\n",
    "vision_start_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_start_token_id)\n",
    "vision_end_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_end_token_id)\n",
    "\n",
    "output_attn: Tuple[Tuple[Tensor, ...], ...] = copy.deepcopy(output_ids.attentions)\n",
    "# get the length of the prefilling and full attention\n",
    "pref_len: int = output_attn[0][0].shape[3]\n",
    "full_len: int = output_attn[-1][0].shape[3]\n",
    "prefill_attn: Tuple[Tensor, ...] = output_attn[0]\n",
    "\n",
    "# batchsize should be 1\n",
    "assert prefill_attn[0].shape[0] == 1\n",
    "full_attn = []\n",
    "for l, layer in enumerate(prefill_attn):\n",
    "    layer = layer.cpu().squeeze(0).float()\n",
    "    layer = F.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "    for i in range(full_len - pref_len):\n",
    "        # print(i, )\n",
    "        # cur_attn = output_attn[i][l].cpu().squeeze(0).float()\n",
    "        cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "        # print(cur_attn.shape)\n",
    "        layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "    full_attn.append(layer)\n",
    "mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "\n",
    "image_output_attn = torch.mean(mean_attn[pref_len:, vision_start_token_idx + 1:vision_end_token_idx], dim=0)\n",
    "\n",
    "def calculate_dynamic_threshold(attn, percentile=98):\n",
    "    hist = torch.histc(attn, bins=100)\n",
    "    cdf = torch.cumsum(hist, dim=0)/torch.sum(hist)\n",
    "    threshold = torch.argmax((cdf > percentile/100).float()).item()/100\n",
    "    return threshold\n",
    "\n",
    "threshold = calculate_dynamic_threshold(image_output_attn)\n",
    "print(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighted_vision_tokens(attn_map, keep_percentage=threshold):\n",
    "    # Get the attention values sorted in descending order\n",
    "    sorted_attention, sorted_indices = torch.sort(attn_map, descending=True)\n",
    "    \n",
    "    # Determine the number of tokens to keep\n",
    "    num_tokens_to_keep = int(len(sorted_attention) * keep_percentage)\n",
    "    \n",
    "    # Create a weight mask where the top tokens have higher weight\n",
    "    weight_vision_token = torch.zeros_like(attn_map, dtype=torch.float)\n",
    "    \n",
    "    # Assign weights for tokens (top tokens get higher weights, others get smaller weights)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(0.6, 1.0, len(sorted_attention) - num_tokens_to_keep)\n",
    "\n",
    "    return weight_vision_token\n",
    "    \n",
    "weight_vision_token = reweighted_vision_tokens(image_output_attn).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vision_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_vision_token.size()\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "print(n_image_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "\n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.type(model.visual.get_dtype())\n",
    "    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "    n_image_features = image_embeds.shape[0]\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(\n",
    "            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        )\n",
    "    image_mask = (\n",
    "        (input_ids == model.config.image_token_id)\n",
    "        .unsqueeze(-1)\n",
    "        .expand_as(inputs_embeds)\n",
    "        .to(inputs_embeds.device)\n",
    "    )\n",
    "    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    print(image_embeds)\n",
    "    image_embeds *= weight_vision_token[:, None]\n",
    "    print(image_embeds)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "print(image_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
