{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"MAX_PIXELS\"]=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5005215396b44303bf6ad646918d89b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "path = 'OpenGVLab/InternVL2-8B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.type of InternVLChatModel(\n",
       "  (vision_model): InternVisionModel(\n",
       "    (embeddings): InternVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InternVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (inner_attn): FlashAttention()\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): InternLM2ForCausalLM(\n",
       "    (model): InternLM2Model(\n",
       "      (tok_embeddings): Embedding(92553, 4096, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x InternLM2DecoderLayer(\n",
       "          (attention): InternLM2FlashAttention2(\n",
       "            (wqkv): Linear(in_features=4096, out_features=6144, bias=False)\n",
       "            (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()\n",
       "          )\n",
       "          (feed_forward): InternLM2MLP(\n",
       "            (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (attention_norm): InternLM2RMSNorm()\n",
       "          (ffn_norm): InternLM2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): InternLM2RMSNorm()\n",
       "    )\n",
       "    (output): Linear(in_features=4096, out_features=92553, bias=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed_weight = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_forward = model.forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLCausalLMOutputWithPast\n",
    "\n",
    "def patch_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        pixel_values_videos: Optional[torch.FloatTensor] = None,\n",
    "        image_grid_thw: Optional[torch.LongTensor] = None,\n",
    "        video_grid_thw: Optional[torch.LongTensor] = None,\n",
    "        rope_deltas: Optional[torch.LongTensor] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "        >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "        >>> messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "            if pixel_values is not None:\n",
    "                pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "                n_image_features = image_embeds.shape[0]\n",
    "                if n_image_tokens != n_image_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "                    )\n",
    "                image_mask = (\n",
    "                    (input_ids == self.config.image_token_id)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand_as(inputs_embeds)\n",
    "                    .to(inputs_embeds.device)\n",
    "                )\n",
    "                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                if self.embed_weight is not None:\n",
    "                    print(image_embeds.shape)\n",
    "                    print(self.embed_weight.shape)\n",
    "                    print(image_embeds)\n",
    "                    print(self.embed_weight)\n",
    "                    image_embeds *= self.embed_weight[:, None]\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "            if pixel_values_videos is not None:\n",
    "                pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n",
    "                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n",
    "                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n",
    "                n_video_features = video_embeds.shape[0]\n",
    "                if n_video_tokens != n_video_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n",
    "                    )\n",
    "                video_mask = (\n",
    "                    (input_ids == self.config.video_token_id)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand_as(inputs_embeds)\n",
    "                    .to(inputs_embeds.device)\n",
    "                )\n",
    "                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n",
    "        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n",
    "            # calculate RoPE index once per generation in the pre-fill stage only\n",
    "            if (\n",
    "                (cache_position is not None and cache_position[0] == 0)\n",
    "                or self.rope_deltas is None\n",
    "                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n",
    "            ):\n",
    "                position_ids, rope_deltas = self.get_rope_index(\n",
    "                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n",
    "                )\n",
    "                self.rope_deltas = rope_deltas\n",
    "            # then use the prev pre-calculated rope-deltas to get the correct position ids\n",
    "            else:\n",
    "                batch_size, seq_length, _ = inputs_embeds.shape\n",
    "                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n",
    "                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n",
    "                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n",
    "                if cache_position is not None:  # otherwise `deltas` is an int `0`\n",
    "                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n",
    "                    delta = delta.to(position_ids.device)\n",
    "                position_ids = position_ids.add(delta)\n",
    "                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return Qwen2VLCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            rope_deltas=self.rope_deltas,\n",
    "        )\n",
    "\n",
    "model.forward = patch_forward.__get__(model, Qwen2VLForConditionalGeneration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are a helpful assistant. Here is one image:\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/image.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Here is another image:\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/sample.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(\n",
    "            **inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def get_mean_attn_weights(output_ids) -> Tensor:\n",
    "    r\"\"\"\n",
    "    get the mean attention weights of the prefilling and full attention\n",
    "    Args:\n",
    "        output_ids: the output ids of the model\n",
    "    Returns:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "    \"\"\"\n",
    "    output_attn = output_ids.attentions\n",
    "    pref_len = output_attn[0][0].shape[3]\n",
    "    full_len = output_attn[-1][0].shape[3]\n",
    "    prefill_attn = output_attn[0]\n",
    "    assert prefill_attn[0].shape[0] == 1, \"batch size should be 1\"\n",
    "    full_attn = []\n",
    "\n",
    "    for l, layer in enumerate(prefill_attn):\n",
    "        layer = layer.cpu().squeeze(0).float()\n",
    "        layer = torch.nn.functional.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "        for i in range(full_len - pref_len):\n",
    "            cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "            layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "        full_attn.append(layer)\n",
    "    mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "    return mean_attn\n",
    "\n",
    "aw = get_mean_attn_weights(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._tensor import Tensor\n",
    "\n",
    "\n",
    "def get_visual_token_attn_weights(mean_attn, inputs) -> Tuple[Tensor, ...]:\n",
    "    r\"\"\"\n",
    "    Get the attention weights of the visual tokens\n",
    "    Args:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "        inputs: the inputs of the model\n",
    "    Returns:\n",
    "        visual_token_attn_weights: the tuple of the attention weights of the visual tokens, each element shape: (V, V)\n",
    "    \"\"\"\n",
    "    assert inputs[\"input_ids\"].shape[0] == 1, \"batch size should be 1\"\n",
    "    pref_len = len(inputs['input_ids'][0])\n",
    "    vision_start_token_indices = torch.where(\n",
    "        inputs[\"input_ids\"][0] == model.config.vision_start_token_id\n",
    "    )[0]\n",
    "    vision_end_token_indices = torch.where(\n",
    "        inputs[\"input_ids\"][0] == model.config.vision_end_token_id\n",
    "    )[0]\n",
    "    # assert len(vision_start_token_indices) == len(vision_end_token_indices), \"vision start and end token idx should be the same\"\n",
    "    # print(vision_start_token_indices)\n",
    "    # print(vision_end_token_indices)\n",
    "    # iterate over multiple images\n",
    "    visual_token_attn_weights = tuple(\n",
    "        torch.mean(mean_attn[pref_len:, s + 1 : e], dim=0)\n",
    "        for s, e in zip(\n",
    "            vision_start_token_indices, vision_end_token_indices, strict=True\n",
    "        )\n",
    "    )\n",
    "    return visual_token_attn_weights\n",
    "\n",
    "\n",
    "vw = get_visual_token_attn_weights(aw, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "# Apply weighted attention to vision tokens, fix multiple images\n",
    "def reweighted_vision_tokens(\n",
    "    vision_attn_weight,\n",
    "    keep_percentage,\n",
    "    weighting_type: Literal[\"linear\", \"uniform\"] = \"linear\",\n",
    "    lowest_weight=0.0,\n",
    "):\n",
    "    sorted_indices = torch.argsort(vision_attn_weight, descending=True)\n",
    "    num_tokens_to_keep = int(len(vision_attn_weight) * keep_percentage)\n",
    "    weight_vision_token = torch.zeros_like(vision_attn_weight, dtype=torch.float)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    if weighting_type == \"linear\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(\n",
    "            lowest_weight, 1.0, len(vision_attn_weight) - num_tokens_to_keep\n",
    "        )\n",
    "    else:\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = lowest_weight\n",
    "    return weight_vision_token\n",
    "\n",
    "vm_linear = [reweighted_vision_tokens(v, 0.6, \"linear\", 0.0) for v in vw]\n",
    "vm_uniform = [reweighted_vision_tokens(v, 0.6, \"uniform\", 0.0) for v in vw]\n",
    "\n",
    "model.embed_weight = torch.concat(vm_linear, dim=0).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and echo output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids.sequences)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from torch import Tensor\n",
    "import copy\n",
    "\n",
    "vision_start_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_start_token_id)\n",
    "vision_end_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_end_token_id)\n",
    "\n",
    "output_attn: Tuple[Tuple[Tensor, ...], ...] = copy.deepcopy(output_ids.attentions)\n",
    "# get the length of the prefilling and full attention\n",
    "pref_len: int = output_attn[0][0].shape[3]\n",
    "full_len: int = output_attn[-1][0].shape[3]\n",
    "prefill_attn: Tuple[Tensor, ...] = output_attn[0]\n",
    "\n",
    "# batchsize should be 1\n",
    "assert prefill_attn[0].shape[0] == 1\n",
    "full_attn = []\n",
    "for l, layer in enumerate(prefill_attn):\n",
    "    layer = layer.cpu().squeeze(0).float()\n",
    "    layer = F.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "    for i in range(full_len - pref_len):\n",
    "        # print(i, )\n",
    "        # cur_attn = output_attn[i][l].cpu().squeeze(0).float()\n",
    "        cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "        # print(cur_attn.shape)\n",
    "        layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "    full_attn.append(layer)\n",
    "mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "\n",
    "image_output_attn = torch.mean(mean_attn[pref_len:, vision_start_token_idx + 1:vision_end_token_idx], dim=0)\n",
    "\n",
    "def calculate_dynamic_threshold(attn, percentile=98):\n",
    "    hist = torch.histc(attn, bins=100)\n",
    "    cdf = torch.cumsum(hist, dim=0)/torch.sum(hist)\n",
    "    threshold = torch.argmax((cdf > percentile/100).float()).item()/100\n",
    "    return threshold\n",
    "\n",
    "threshold = calculate_dynamic_threshold(image_output_attn)\n",
    "print(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighted_vision_tokens(attn_map, keep_percentage=threshold):\n",
    "    # Get the attention values sorted in descending order\n",
    "    sorted_attention, sorted_indices = torch.sort(attn_map, descending=True)\n",
    "    \n",
    "    # Determine the number of tokens to keep\n",
    "    num_tokens_to_keep = int(len(sorted_attention) * keep_percentage)\n",
    "    \n",
    "    # Create a weight mask where the top tokens have higher weight\n",
    "    weight_vision_token = torch.zeros_like(attn_map, dtype=torch.float)\n",
    "    \n",
    "    # Assign weights for tokens (top tokens get higher weights, others get smaller weights)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(0.6, 1.0, len(sorted_attention) - num_tokens_to_keep)\n",
    "\n",
    "    return weight_vision_token\n",
    "    \n",
    "weight_vision_token = reweighted_vision_tokens(image_output_attn).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vision_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_vision_token.size()\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "print(n_image_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "\n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.type(model.visual.get_dtype())\n",
    "    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "    n_image_features = image_embeds.shape[0]\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(\n",
    "            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        )\n",
    "    image_mask = (\n",
    "        (input_ids == model.config.image_token_id)\n",
    "        .unsqueeze(-1)\n",
    "        .expand_as(inputs_embeds)\n",
    "        .to(inputs_embeds.device)\n",
    "    )\n",
    "    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    print(image_embeds)\n",
    "    image_embeds *= weight_vision_token[:, None]\n",
    "    print(image_embeds)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "print(image_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmabsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
