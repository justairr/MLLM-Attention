{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"MAX_PIXELS\"]=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5e78a4e7a64211a352f76e278b67a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    " \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    " torch_dtype=torch.bfloat16,\n",
    " attn_implementation=\"eager\",\n",
    " device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed_weight = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_forward = model.forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLCausalLMOutputWithPast\n",
    "\n",
    "def patch_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        pixel_values_videos: Optional[torch.FloatTensor] = None,\n",
    "        image_grid_thw: Optional[torch.LongTensor] = None,\n",
    "        video_grid_thw: Optional[torch.LongTensor] = None,\n",
    "        rope_deltas: Optional[torch.LongTensor] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "        >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "        >>> messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "            if pixel_values is not None:\n",
    "                pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "                n_image_features = image_embeds.shape[0]\n",
    "                if n_image_tokens != n_image_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "                    )\n",
    "                image_mask = (\n",
    "                    (input_ids == self.config.image_token_id)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand_as(inputs_embeds)\n",
    "                    .to(inputs_embeds.device)\n",
    "                )\n",
    "                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                if self.embed_weight is not None:\n",
    "                    print(image_embeds.shape)\n",
    "                    print(self.embed_weight.shape)\n",
    "                    print(image_embeds)\n",
    "                    print(self.embed_weight)\n",
    "                    image_embeds *= self.embed_weight[:, None]\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "            if pixel_values_videos is not None:\n",
    "                pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n",
    "                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n",
    "                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n",
    "                n_video_features = video_embeds.shape[0]\n",
    "                if n_video_tokens != n_video_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n",
    "                    )\n",
    "                video_mask = (\n",
    "                    (input_ids == self.config.video_token_id)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand_as(inputs_embeds)\n",
    "                    .to(inputs_embeds.device)\n",
    "                )\n",
    "                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n",
    "        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n",
    "            # calculate RoPE index once per generation in the pre-fill stage only\n",
    "            if (\n",
    "                (cache_position is not None and cache_position[0] == 0)\n",
    "                or self.rope_deltas is None\n",
    "                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n",
    "            ):\n",
    "                position_ids, rope_deltas = self.get_rope_index(\n",
    "                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n",
    "                )\n",
    "                self.rope_deltas = rope_deltas\n",
    "            # then use the prev pre-calculated rope-deltas to get the correct position ids\n",
    "            else:\n",
    "                batch_size, seq_length, _ = inputs_embeds.shape\n",
    "                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n",
    "                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n",
    "                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n",
    "                if cache_position is not None:  # otherwise `deltas` is an int `0`\n",
    "                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n",
    "                    delta = delta.to(position_ids.device)\n",
    "                position_ids = position_ids.add(delta)\n",
    "                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return Qwen2VLCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            rope_deltas=self.rope_deltas,\n",
    "        )\n",
    "\n",
    "model.forward = patch_forward.__get__(model, Qwen2VLForConditionalGeneration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are a helpful assistant. Here is one image:\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/image.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Here is another image:\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/sample.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "#print(image_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_images(image_list, mode=\"noise\", color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    根据 `mode` 生成与 `image_list` 中图像等大的随机噪声或纯色图像。\n",
    "\n",
    "    参数：\n",
    "    - image_list: list[PIL.Image]，输入的图像列表。\n",
    "    - mode: str，\"noise\" 生成随机噪声图像，\"blank\" 生成纯色图像。\n",
    "    - color: tuple，生成纯色图像时的颜色，默认为白色 (255, 255, 255)。\n",
    "\n",
    "    返回：\n",
    "    - list[PIL.Image]，生成的图像列表。\n",
    "    \"\"\"\n",
    "    generated_images = []\n",
    "    \n",
    "    for img in image_list:\n",
    "        width, height = img.size\n",
    "        \n",
    "        if mode == \"noise\":\n",
    "            noise_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n",
    "            generated_image = Image.fromarray(noise_array)\n",
    "        elif mode == \"blank\":\n",
    "            generated_image = Image.new(\"RGB\", (width, height), color)\n",
    "        else:\n",
    "            raise ValueError(\"mode could only be 'noise' or 'blank'\")\n",
    "        \n",
    "        generated_images.append(generated_image)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "neg_image_inputs = generate_images(image_inputs, mode=\"noise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scm/miniconda3/envs/llmabsa/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "output_ids = model.generate(\n",
    "            **inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0039,  1.0544,  1.1712,  ...,  0.9230, -0.5417,  2.0179],\n",
      "        [ 0.0179,  1.3026,  0.1639,  ...,  0.5959,  0.1693,  0.5532],\n",
      "        [ 1.0982, -0.1280,  1.9157,  ...,  0.4964,  0.1409,  0.2120],\n",
      "        ...,\n",
      "        [-0.7266, -1.3835,  1.8281,  ..., -1.1958,  1.2074, -1.2527],\n",
      "        [-0.6828, -1.4857,  0.0909,  ...,  0.6670, -0.3284,  0.3257],\n",
      "        [-1.5879,  0.0325, -0.2010,  ...,  0.8945,  1.7335,  1.3069]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "neg_inputs = processor(\n",
    "    text=[text],\n",
    "    images=neg_image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "neg_inputs = neg_inputs.to(\"cuda\")\n",
    "print(neg_inputs.pixel_values)\n",
    "negative_output_ids = model.generate(\n",
    "            **neg_inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to be a pattern of colorful, pixelated dots. The dots are of various colors, including red, blue, green, yellow, and purple, and they are arranged in a seemingly random fashion across the entire image. The overall effect is a textured, noisy appearance, reminiscent of static on a television screen or a digital glitch.\n"
     ]
    }
   ],
   "source": [
    "# If you wanna check what's the output of negative generation, run me.\n",
    "\n",
    "generated_ids = negative_output_ids.sequences\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def get_mean_attn_weights(output_ids) -> Tensor:\n",
    "    r\"\"\"\n",
    "    get the mean attention weights of the prefilling and full attention\n",
    "    Args:\n",
    "        output_ids: the output ids of the model\n",
    "    Returns:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "    \"\"\"\n",
    "    output_attn = output_ids.attentions\n",
    "    pref_len = output_attn[0][0].shape[3]\n",
    "    full_len = output_attn[-1][0].shape[3]\n",
    "    prefill_attn = output_attn[0]\n",
    "    assert prefill_attn[0].shape[0] == 1, \"batch size should be 1\"\n",
    "    full_attn = []\n",
    "\n",
    "    for l, layer in enumerate(prefill_attn):\n",
    "        layer = layer.cpu().squeeze(0).float()\n",
    "        layer = torch.nn.functional.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "        for i in range(full_len - pref_len):\n",
    "            cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "            layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "        full_attn.append(layer)\n",
    "    mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "    return mean_attn\n",
    "\n",
    "aw = get_mean_attn_weights(output_ids)\n",
    "\n",
    "neg_aw = get_mean_attn_weights(negative_output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._tensor import Tensor\n",
    "\n",
    "\n",
    "def get_visual_token_attn_weights(mean_attn, inputs) -> Tuple[Tensor, ...]:\n",
    "    r\"\"\"\n",
    "    Get the attention weights of the visual tokens\n",
    "    Args:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "        inputs: the inputs of the model\n",
    "    Returns:\n",
    "        visual_token_attn_weights: the tuple of the attention weights of the visual tokens, each element shape: (V, V)\n",
    "    \"\"\"\n",
    "    assert inputs[\"input_ids\"].shape[0] == 1, \"batch size should be 1\"\n",
    "    pref_len = len(inputs['input_ids'][0])\n",
    "    vision_start_token_indices = torch.where(\n",
    "        inputs[\"input_ids\"][0] == model.config.vision_start_token_id\n",
    "    )[0]\n",
    "    vision_end_token_indices = torch.where(\n",
    "        inputs[\"input_ids\"][0] == model.config.vision_end_token_id\n",
    "    )[0]\n",
    "    # assert len(vision_start_token_indices) == len(vision_end_token_indices), \"vision start and end token idx should be the same\"\n",
    "    # print(vision_start_token_indices)\n",
    "    # print(vision_end_token_indices)\n",
    "    # iterate over multiple images\n",
    "    visual_token_attn_weights = tuple(\n",
    "        torch.mean(mean_attn[pref_len:, s + 1 : e], dim=0)\n",
    "        for s, e in zip(\n",
    "            vision_start_token_indices, vision_end_token_indices, strict=True\n",
    "        )\n",
    "    )\n",
    "    return visual_token_attn_weights\n",
    "\n",
    "\n",
    "vw = get_visual_token_attn_weights(aw, inputs)\n",
    "neg_vw = get_visual_token_attn_weights(neg_aw, neg_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "# Apply weighted attention to vision tokens, fix multiple images\n",
    "def reweighted_vision_tokens(\n",
    "    vision_attn_weight,\n",
    "    keep_percentage,\n",
    "    weighting_type: Literal[\"linear\", \"uniform\", \"suppress\"] = \"linear\",\n",
    "    lowest_weight=0.0,\n",
    "    neg_attn_weight=None,\n",
    "    suppress_alpha=0.5,\n",
    "):\n",
    "    if weighting_type == \"suppress\":\n",
    "        if neg_attn_weight is None:\n",
    "            raise ValueError(\"neg_attn_weight must be provided for suppress mode\")\n",
    "        # 使用负样例注意力权重进行抑制\n",
    "        weight_vision_token = 1 - suppress_alpha * neg_attn_weight\n",
    "        return weight_vision_token\n",
    "\n",
    "    sorted_indices = torch.argsort(vision_attn_weight, descending=True)\n",
    "    num_tokens_to_keep = int(len(vision_attn_weight) * keep_percentage)\n",
    "    weight_vision_token = torch.zeros_like(vision_attn_weight, dtype=torch.float)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    if weighting_type == \"linear\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(\n",
    "            lowest_weight, 1.0, len(vision_attn_weight) - num_tokens_to_keep\n",
    "        )\n",
    "    else:\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = lowest_weight\n",
    "    return weight_vision_token\n",
    "\n",
    "#vm_linear = [reweighted_vision_tokens(v, 0.6, \"linear\", 0.0) for v in vw]\n",
    "#vm_uniform = [reweighted_vision_tokens(v, 0.6, \"uniform\", 0.0) for v in vw]\n",
    "\n",
    "vm_suppress = [reweighted_vision_tokens(v, 1, \"suppress\", 0, neg_v) for v, neg_v in zip(vw, neg_vw)]\n",
    "\n",
    "model.embed_weight = torch.concat(vm_suppress, dim=0).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1106, 3584])\n",
      "torch.Size([1106])\n",
      "tensor([[ 0.7578, -2.0000,  3.7344,  ...,  3.2031,  1.5234,  0.8125],\n",
      "        [ 0.5547,  0.2139,  0.1289,  ...,  0.5469,  0.4688,  0.5352],\n",
      "        [-0.0061, -0.0557,  0.1592,  ...,  0.1079,  0.1094,  0.3008],\n",
      "        ...,\n",
      "        [-0.3359,  0.1328,  0.1572,  ..., -0.0330, -0.2812,  0.1621],\n",
      "        [ 0.2637, -0.1807,  0.6016,  ..., -0.1436, -0.0898,  0.3027],\n",
      "        [-3.7344, -3.9219, -0.7695,  ...,  1.6250, -0.0908, -3.7031]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.9998, 0.9998, 0.9996,  ..., 1.0000, 1.0000, 0.9998], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a collage of eight separate photographs arranged in a \\(2 2 \\\\) grid. Each photograph features a different animal or natural scene. Below is a detailed description of each photograph in the the image:\n",
      "\n",
      "1. **Top Left**: This of a dog with long, wavy fur. The dog appears to be a Golden Retriever or a similar breed, with a light golden-brown coat. The dog is lying down on a grassy, and its ears are flopped over. The background is blurred, but it appears to be an outdoor setting with some greenery.\n",
      "\n",
      "2. **Top Middle**: photograph of a\n"
     ]
    }
   ],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and echo output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids.sequences)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from torch import Tensor\n",
    "import copy\n",
    "\n",
    "vision_start_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_start_token_id)\n",
    "vision_end_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_end_token_id)\n",
    "\n",
    "output_attn: Tuple[Tuple[Tensor, ...], ...] = copy.deepcopy(output_ids.attentions)\n",
    "# get the length of the prefilling and full attention\n",
    "pref_len: int = output_attn[0][0].shape[3]\n",
    "full_len: int = output_attn[-1][0].shape[3]\n",
    "prefill_attn: Tuple[Tensor, ...] = output_attn[0]\n",
    "\n",
    "# batchsize should be 1\n",
    "assert prefill_attn[0].shape[0] == 1\n",
    "full_attn = []\n",
    "for l, layer in enumerate(prefill_attn):\n",
    "    layer = layer.cpu().squeeze(0).float()\n",
    "    layer = F.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "    for i in range(full_len - pref_len):\n",
    "        # print(i, )\n",
    "        # cur_attn = output_attn[i][l].cpu().squeeze(0).float()\n",
    "        cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "        # print(cur_attn.shape)\n",
    "        layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "    full_attn.append(layer)\n",
    "mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "\n",
    "image_output_attn = torch.mean(mean_attn[pref_len:, vision_start_token_idx + 1:vision_end_token_idx], dim=0)\n",
    "\n",
    "def calculate_dynamic_threshold(attn, percentile=98):\n",
    "    hist = torch.histc(attn, bins=100)\n",
    "    cdf = torch.cumsum(hist, dim=0)/torch.sum(hist)\n",
    "    threshold = torch.argmax((cdf > percentile/100).float()).item()/100\n",
    "    return threshold\n",
    "\n",
    "threshold = calculate_dynamic_threshold(image_output_attn)\n",
    "print(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighted_vision_tokens(attn_map, keep_percentage=threshold):\n",
    "    # Get the attention values sorted in descending order\n",
    "    sorted_attention, sorted_indices = torch.sort(attn_map, descending=True)\n",
    "    \n",
    "    # Determine the number of tokens to keep\n",
    "    num_tokens_to_keep = int(len(sorted_attention) * keep_percentage)\n",
    "    \n",
    "    # Create a weight mask where the top tokens have higher weight\n",
    "    weight_vision_token = torch.zeros_like(attn_map, dtype=torch.float)\n",
    "    \n",
    "    # Assign weights for tokens (top tokens get higher weights, others get smaller weights)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(0.6, 1.0, len(sorted_attention) - num_tokens_to_keep)\n",
    "\n",
    "    return weight_vision_token\n",
    "    \n",
    "weight_vision_token = reweighted_vision_tokens(image_output_attn).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vision_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_vision_token.size()\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "print(n_image_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "\n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.type(model.visual.get_dtype())\n",
    "    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "    n_image_features = image_embeds.shape[0]\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(\n",
    "            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        )\n",
    "    image_mask = (\n",
    "        (input_ids == model.config.image_token_id)\n",
    "        .unsqueeze(-1)\n",
    "        .expand_as(inputs_embeds)\n",
    "        .to(inputs_embeds.device)\n",
    "    )\n",
    "    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    print(image_embeds)\n",
    "    image_embeds *= weight_vision_token[:, None]\n",
    "    print(image_embeds)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "print(image_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmabsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
