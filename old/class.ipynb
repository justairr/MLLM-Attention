{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys, inspect\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Qwen2VLProcessor, Qwen2VLForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Qwen2VLInference:\n",
    "    def __init__(self, model_name: str, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            attn_implementation=\"eager\",\n",
    "        ).to(device)\n",
    "        self.processor = Qwen2VLProcessor.from_pretrained(model_name)\n",
    "        self.apply_monkey_patch()\n",
    "\n",
    "    def apply_monkey_patch(self):\n",
    "        \"\"\"\n",
    "        Apply the monkey patch to modify model behavior.\n",
    "        \"\"\"\n",
    "        from qwen_mod import get_rope_index_modified  # Adjust this to your module's structure\n",
    "        self.model.get_rope_index = get_rope_index_modified.__get__(self.model)\n",
    "\n",
    "    def preprocess_inputs(self, image_path: str, conversation: List[dict]):\n",
    "        image = Image.open(image_path)\n",
    "        text_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = self.processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "        return inputs.to(self.device), text_prompt\n",
    "\n",
    "    def generate_output(self, inputs, max_new_tokens=512):\n",
    "        with torch.no_grad():\n",
    "            return self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_hidden_states=True,\n",
    "                output_attentions=True,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "    def calculate_attention(self, output_attn, pref_len, full_len):\n",
    "        prefill_attn = output_attn[0]\n",
    "        assert prefill_attn[0].shape[0] == 1\n",
    "        full_attn = []\n",
    "        for l, layer in enumerate(prefill_attn):\n",
    "            layer = layer.cpu().squeeze(0).float()\n",
    "            layer = F.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "            for i in range(full_len - pref_len):\n",
    "                cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "                layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "            full_attn.append(layer)\n",
    "        return torch.stack(full_attn).mean(dim=(0, 1))\n",
    "\n",
    "    def filter_vision_tokens(self, inputs, mean_attn, vision_start_idx, vision_end_idx, keep_ratio=0.6):\n",
    "        image_attention = torch.mean(mean_attn[vision_start_idx:vision_end_idx], dim=0)\n",
    "        _, top_indices = image_attention.topk(int(len(image_attention) * keep_ratio))\n",
    "        top_indices += vision_start_idx\n",
    "        for i in range(vision_start_idx, vision_end_idx):\n",
    "            if i in top_indices:\n",
    "                inputs[\"attention_mask\"][0, i] = True\n",
    "            else:\n",
    "                inputs[\"attention_mask\"][0, i] = False\n",
    "        return inputs\n",
    "\n",
    "    def visualize_attention(self, mean_attn, full_len):\n",
    "        sqrt_attn_map = np.sqrt(mean_attn.numpy())\n",
    "        plt.imshow(sqrt_attn_map[1:, 1:], cmap=\"inferno\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    def run_two_stage_inference(self, image_path: str, conversation: List[dict], keep_ratio=0.6):\n",
    "        # Stage 1: Initial inference\n",
    "        inputs, text_prompt = self.preprocess_inputs(image_path, conversation)\n",
    "        output_ids = self.generate_output(inputs)\n",
    "        output_attn = output_ids.attentions\n",
    "        pref_len = output_attn[0][0].shape[3]\n",
    "        full_len = output_attn[-1][0].shape[3]\n",
    "\n",
    "        # Calculate attention map\n",
    "        mean_attn = self.calculate_attention(output_attn, pref_len, full_len)\n",
    "\n",
    "        # Visualize attention (optional)\n",
    "        #self.visualize_attention(mean_attn, full_len)\n",
    "\n",
    "        # Filter vision tokens based on attention\n",
    "        vision_start_idx = inputs[\"input_ids\"][0].tolist().index(self.model.config.vision_start_token_id)\n",
    "        vision_end_idx = inputs[\"input_ids\"][0].tolist().index(self.model.config.vision_end_token_id)\n",
    "        inputs = self.filter_vision_tokens(inputs, mean_attn, vision_start_idx, vision_end_idx, keep_ratio)\n",
    "\n",
    "        # Stage 2: Inference with filtered tokens\n",
    "        refined_output_ids = self.generate_output(inputs)\n",
    "        refined_text = self.processor.batch_decode(\n",
    "            refined_output_ids.sequences, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        return refined_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3109054e64944a7aaa8d9ea555ab8cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scm/miniconda3/envs/llmabsa/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: [\"system\\nYou are a helpful assistant.\\nuser\\nFind the missing amounts for company B. Options are ['$63,020', '$58,410', '$71,320', '$77,490']\\nassistant\\nTo find the missing amounts for Company B, we need to balance the income statement. The income statement is structured as follows:\\n\\n\\\\[ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\\"]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "    handler = Qwen2VLInference(model_name)\n",
    "    \n",
    "    image_path = \"examples/image.png\"\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Find the missing amounts for company B. Options are ['$63,020', '$58,410', '$71,320', '$77,490']\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    result = handler.run_two_stage_inference(image_path, conversation, keep_ratio=0.6)\n",
    "    print(\"Final Output:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"MMMU/MMMU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmabsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
