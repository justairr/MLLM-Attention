{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cad41187da457db2ae497671db623e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from qwen import Qwen2VLForAttnExtraction\n",
    "\n",
    "small_model = Qwen2VLForAttnExtraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = small_model.extract_attention(\n",
    "    \"Where's the fox and dog in the image?\",\n",
    "    Image.open(\"/home/scm/MLLM-Attention/examples/sample.png\"),\n",
    "    # [],\n",
    "    attn_type=\"contrastive\",\n",
    "    single_token_generation=True,\n",
    "    contrast_layers=(14, 6),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 37])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a033b7b9a76f45f5bbebc0edcf2a1fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    " \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    " torch_dtype=torch.bfloat16,\n",
    " attn_implementation=\"eager\",\n",
    " device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed_weight = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_forward = model.forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLCausalLMOutputWithPast\n",
    "\n",
    "def patch_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        pixel_values_videos: Optional[torch.FloatTensor] = None,\n",
    "        image_grid_thw: Optional[torch.LongTensor] = None,\n",
    "        video_grid_thw: Optional[torch.LongTensor] = None,\n",
    "        rope_deltas: Optional[torch.LongTensor] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "        >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "        >>> messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "            if pixel_values is not None:\n",
    "                pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "                n_image_features = image_embeds.shape[0]\n",
    "                if n_image_tokens != n_image_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "                    )\n",
    "                image_mask = (\n",
    "                    (input_ids == self.config.image_token_id)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand_as(inputs_embeds)\n",
    "                    .to(inputs_embeds.device)\n",
    "                )\n",
    "                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                #print(image_embeds.shape)\n",
    "                if self.embed_weight is not None:\n",
    "                    #print(image_embeds.shape)\n",
    "                    #print(self.embed_weight.shape)\n",
    "                    #print(image_embeds)\n",
    "                    #print(self.embed_weight)\n",
    "                    image_embeds *= self.embed_weight[:, None]\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "            if pixel_values_videos is not None:\n",
    "                pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n",
    "                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n",
    "                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n",
    "                n_video_features = video_embeds.shape[0]\n",
    "                if n_video_tokens != n_video_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n",
    "                    )\n",
    "                video_mask = (\n",
    "                    (input_ids == self.config.video_token_id)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand_as(inputs_embeds)\n",
    "                    .to(inputs_embeds.device)\n",
    "                )\n",
    "                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n",
    "        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n",
    "            # calculate RoPE index once per generation in the pre-fill stage only\n",
    "            if (\n",
    "                (cache_position is not None and cache_position[0] == 0)\n",
    "                or self.rope_deltas is None\n",
    "                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n",
    "            ):\n",
    "                position_ids, rope_deltas = self.get_rope_index(\n",
    "                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n",
    "                )\n",
    "                self.rope_deltas = rope_deltas\n",
    "            # then use the prev pre-calculated rope-deltas to get the correct position ids\n",
    "            else:\n",
    "                batch_size, seq_length, _ = inputs_embeds.shape\n",
    "                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n",
    "                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n",
    "                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n",
    "                if cache_position is not None:  # otherwise `deltas` is an int `0`\n",
    "                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n",
    "                    delta = delta.to(position_ids.device)\n",
    "                position_ids = position_ids.add(delta)\n",
    "                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return Qwen2VLCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            rope_deltas=self.rope_deltas,\n",
    "        )\n",
    "\n",
    "model.forward = patch_forward.__get__(model, Qwen2VLForConditionalGeneration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/home/scm/MLLM-Attention/examples/sample.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Where's the fox in this image?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "#print(image_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scm/miniconda3/envs/llmabsa/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([666, 3584])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "output_ids = model.generate(\n",
    "            **inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 37])\n"
     ]
    }
   ],
   "source": [
    "print(attn[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Apply weighted attention to vision tokens, fix multiple images\n",
    "def reweighted_vision_tokens(\n",
    "    vision_attn_weight,\n",
    "    keep_percentage,\n",
    "    weighting_type: Literal[\"linear\", \"uniform\", \"suppress\"] = \"linear\",\n",
    "    lowest_weight=0.0,\n",
    "    neg_attn_weight=None,\n",
    "    suppress_alpha=0.5,\n",
    "):\n",
    "    if weighting_type == \"suppress\":\n",
    "        if neg_attn_weight is None:\n",
    "            raise ValueError(\"neg_attn_weight must be provided for suppress mode\")\n",
    "        # 使用负样例注意力权重进行抑制\n",
    "        weight_vision_token = 1 - suppress_alpha * neg_attn_weight\n",
    "        return weight_vision_token\n",
    "\n",
    "    sorted_indices = torch.argsort(vision_attn_weight, descending=True)\n",
    "    num_tokens_to_keep = int(len(vision_attn_weight) * keep_percentage)\n",
    "    weight_vision_token = torch.zeros_like(vision_attn_weight, dtype=torch.float)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    if weighting_type == \"linear\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(\n",
    "            lowest_weight, 1.0, len(vision_attn_weight) - num_tokens_to_keep\n",
    "        )\n",
    "    else:\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = lowest_weight\n",
    "    return weight_vision_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_flat = attn[0].flatten()\n",
    "\n",
    "weight = reweighted_vision_tokens(attn_flat, keep_percentage=0.5, weighting_type=\"linear\")\n",
    "\n",
    "model.embed_weight = weight.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([666, 3584])\n",
      "torch.Size([666, 3584])\n",
      "torch.Size([666])\n",
      "tensor([[-0.6445, -1.9922, -0.0280,  ...,  0.5156,  5.4375,  2.5156],\n",
      "        [ 0.0593, -2.5312, -0.5156,  ...,  3.9375,  4.5000, -0.4121],\n",
      "        [ 1.0625, -1.3750,  1.6094,  ...,  4.2812,  3.3750, -2.8281],\n",
      "        ...,\n",
      "        [-0.3379,  0.1533,  0.1553,  ..., -0.0071, -0.2832,  0.1582],\n",
      "        [ 0.2617, -0.2012,  0.5938,  ..., -0.1367, -0.0972,  0.3086],\n",
      "        [-3.7188, -3.9219, -0.7461,  ...,  1.6484, -0.1172, -3.7031]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([1.0000, 1.0000, 0.6807, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6657, 0.6627,\n",
      "        0.6295, 0.6355, 0.6325, 0.7470, 0.7410, 0.7349, 0.7319, 0.7169, 0.7139,\n",
      "        0.4367, 1.0000, 0.1687, 1.0000, 0.5633, 0.5241, 0.6265, 0.6205, 0.6175,\n",
      "        0.5482, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0572, 1.0000, 1.0000, 0.2078, 1.0000, 1.0000,\n",
      "        1.0000, 0.5813, 1.0000, 1.0000, 0.1265, 1.0000, 1.0000, 1.0000, 0.9127,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.3373, 0.4096, 0.1024, 0.0090,\n",
      "        0.9789, 0.9669, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0873, 0.1777,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0602, 1.0000, 1.0000, 1.0000,\n",
      "        0.2922, 0.7922, 1.0000, 0.2500, 0.2560, 1.0000, 0.7741, 1.0000, 0.3494,\n",
      "        0.3434, 0.4307, 1.0000, 1.0000, 0.8614, 1.0000, 1.0000, 0.1205, 1.0000,\n",
      "        0.2982, 1.0000, 0.8554, 1.0000, 1.0000, 0.2289, 0.3042, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0060, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.4127, 0.2470, 0.4910, 1.0000, 0.7560, 0.7651, 0.7801, 1.0000,\n",
      "        1.0000, 0.0783, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2741,\n",
      "        1.0000, 1.0000, 1.0000, 0.8072, 0.3554, 1.0000, 0.8855, 0.8886, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8916, 1.0000,\n",
      "        0.5723, 1.0000, 0.3283, 0.2018, 1.0000, 1.0000, 0.1627, 0.5873, 1.0000,\n",
      "        1.0000, 0.3675, 0.6054, 1.0000, 0.1506, 1.0000, 0.6235, 0.3313, 0.5994,\n",
      "        0.5211, 0.0663, 1.0000, 0.1807, 0.4608, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.3404, 1.0000, 0.0120, 1.0000, 1.0000, 1.0000, 0.5422, 1.0000,\n",
      "        1.0000, 1.0000, 0.0964, 1.0000, 0.3946, 1.0000, 1.0000, 1.0000, 0.1898,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.0843, 1.0000, 1.0000, 0.5512, 0.5602,\n",
      "        0.5663, 0.7199, 1.0000, 0.1114, 0.3584, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        0.0211, 1.0000, 0.0753, 0.3645, 0.6958, 1.0000, 0.4880, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.6145, 1.0000, 1.0000, 1.0000, 0.7048,\n",
      "        0.3464, 1.0000, 0.7108, 0.2892, 1.0000, 1.0000, 1.0000, 1.0000, 0.0271,\n",
      "        0.7289, 0.0331, 0.3102, 1.0000, 0.0000, 0.2259, 1.0000, 0.0693, 0.3133,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.4398, 0.4518, 1.0000, 0.4729, 0.7440,\n",
      "        0.6596, 1.0000, 0.2380, 0.2801, 1.0000, 1.0000, 0.0361, 0.6747, 0.2169,\n",
      "        0.2410, 0.2530, 0.6837, 1.0000, 1.0000, 0.6777, 0.5000, 0.1476, 0.2590,\n",
      "        1.0000, 0.6717, 0.1295, 0.3012, 0.6386, 0.0301, 0.1867, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        0.4337, 0.2108, 0.2048, 0.7380, 0.2199, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.3253, 0.2861, 1.0000, 1.0000, 0.3735, 1.0000, 0.0030, 1.0000,\n",
      "        0.0241, 0.7259, 0.7229, 0.3223, 1.0000, 0.6898, 0.0151, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0723, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6928, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.5542, 1.0000, 1.0000, 0.5361, 1.0000, 1.0000, 0.0934,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5392, 1.0000, 1.0000,\n",
      "        1.0000, 0.2229, 0.4488, 0.3193, 0.5331, 1.0000, 0.5181, 0.0181, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6114, 1.0000, 1.0000, 0.4789,\n",
      "        1.0000, 0.4548, 0.0904, 1.0000, 0.6024, 0.1928, 0.5904, 0.5060, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.4578, 0.5783, 1.0000, 0.5753, 0.2319, 1.0000,\n",
      "        1.0000, 1.0000, 0.9367, 1.0000, 0.9217, 1.0000, 0.8765, 0.8976, 0.8946,\n",
      "        1.0000, 0.1988, 0.1596, 0.1145, 0.3825, 1.0000, 1.0000, 1.0000, 0.1175,\n",
      "        0.0813, 1.0000, 1.0000, 0.8795, 0.9970, 0.9880, 0.9398, 0.9699, 0.9639,\n",
      "        0.9608, 0.9578, 1.0000, 0.4940, 0.1446, 1.0000, 0.1325, 0.8042, 1.0000,\n",
      "        0.8012, 0.3976, 1.0000, 1.0000, 0.0633, 1.0000, 0.7982, 0.7892, 1.0000,\n",
      "        0.7861, 1.0000, 0.2349, 0.0542, 1.0000, 1.0000, 1.0000, 0.7500, 0.2620,\n",
      "        0.1235, 1.0000, 1.0000, 1.0000, 0.7530, 0.7831, 0.8705, 0.8675, 0.8645,\n",
      "        0.8584, 0.4849, 0.4187, 1.0000, 0.4699, 1.0000, 0.2651, 1.0000, 1.0000,\n",
      "        1.0000, 0.2771, 0.3916, 1.0000, 0.8313, 0.8283, 0.0482, 0.5030, 1.0000,\n",
      "        0.8163, 0.9307, 0.4066, 0.3855, 1.0000, 0.3614, 0.8193, 0.8223, 0.4759,\n",
      "        0.8253, 0.3524, 1.0000, 0.5120, 1.0000, 0.8343, 0.8373, 0.8404, 0.8434,\n",
      "        0.8133, 0.8494, 0.8524, 1.0000, 1.0000, 1.0000, 0.0452, 1.0000, 0.5090,\n",
      "        0.0512, 1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 0.7590, 0.7620,\n",
      "        0.3795, 0.7681, 0.7711, 1.0000, 0.7771, 1.0000, 0.0422, 0.4247, 1.0000,\n",
      "        1.0000, 0.7952, 0.1386, 1.0000, 1.0000, 0.1747, 0.8102, 0.8464, 0.9428,\n",
      "        0.9458, 0.9488, 0.9518, 1.0000, 0.9548, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.1536, 0.9759, 1.0000, 0.9819, 0.9849, 0.1054, 0.9910, 0.9940,\n",
      "        1.0000, 1.0000, 0.9096, 1.0000, 0.8825, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        0.1958, 0.9006, 0.9036, 0.9066, 1.0000, 1.0000, 0.9157, 0.9187, 0.4157,\n",
      "        0.9247, 0.9277, 0.7018, 0.9337, 0.4217, 0.9729, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.5843, 1.0000, 0.5151, 0.1355, 0.3705, 0.5934, 0.5964, 0.5693,\n",
      "        1.0000, 0.3765, 0.6084, 0.3886, 1.0000, 1.0000, 1.0000, 1.0000, 0.0994,\n",
      "        1.0000, 0.2711, 0.5301, 0.5271, 0.1717, 0.3343, 1.0000, 1.0000, 0.2681,\n",
      "        1.0000, 0.5452, 0.4639, 0.2139, 0.4428, 0.5572, 0.4036, 1.0000, 0.0392,\n",
      "        1.0000, 1.0000, 0.4277, 0.6988, 1.0000, 0.4669, 1.0000, 0.7078, 0.3072,\n",
      "        1.0000, 1.0000, 1.0000, 0.1566, 0.2831, 0.4819, 0.1657, 1.0000, 1.0000,\n",
      "        0.2440, 1.0000, 0.4006, 0.2952, 1.0000, 0.1084, 1.0000, 1.0000, 0.1837,\n",
      "        0.6416, 0.6446, 0.6476, 0.6506, 0.6536, 0.6566, 1.0000, 1.0000, 1.0000,\n",
      "        0.4970, 0.6687, 1.0000, 0.1416, 0.3163, 0.4458, 1.0000, 1.0000, 0.6867],\n",
      "       device='cuda:0')\n",
      "The fox is located in the bottom right corner of the image.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmabsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
