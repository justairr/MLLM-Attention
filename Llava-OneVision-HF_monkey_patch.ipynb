{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoProcessor,\n",
    "  LlavaOnevisionForConditionalGeneration\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521374487fe49539fd96b7bda875a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-onevision-qwen2-7b-ov-chat-hf\"\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(0)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "  model_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user \n",
      "What are these?assistant\n",
      "The image you've provided is a collage of various photographs. From left to right, top to bottom:\n",
      "\n",
      "1. A golden retriever dog looking relaxed.\n",
      "2. A beaver swimming in water among rocks.\n",
      "3. A red panda resting on a rock.\n",
      "4. A geyser erupting with steam and water shooting into the air.\n",
      "5. A colorful bird, possibly a macaw, perched on a branch.\n",
      "6. A waterfall cascading over rocks.\n",
      "7. A hot air balloon with a pilot inside, floating in the sky.\n",
      "8. A white arctic fox walking in snow.\n",
      "\n",
      "Each photograph captures a different aspect of wildlife, nature, and human activity. The collage as a whole showcases the diversity of the natural world.\n"
     ]
    }
   ],
   "source": [
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "raw_image = Image.open(\"examples/sample.png\")\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(\n",
    "    0, torch.float16\n",
    ")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patcher from Qwen2VL\n",
    "from typing import List, Optional, Tuple, Union, Literal\n",
    "import torch\n",
    "from transformers.models.llava_onevision import LlavaOnevisionForConditionalGeneration\n",
    "\n",
    "# Calculate dynamic threshold for attention map\n",
    "def calculate_dynamic_threshold(visual_token_attn_score, percentile=95):\n",
    "    hist = torch.histc(visual_token_attn_score, bins=100)\n",
    "    cdf = torch.cumsum(hist, dim=0) / torch.sum(hist)\n",
    "    threshold = torch.argmax((cdf > percentile / 100).float()).item() / 100\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def get_mean_attn_score(output_ids) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    get the mean attention weights of the prefilling and full attention\n",
    "    Args:\n",
    "        output_ids: the output ids of the model\n",
    "    Returns:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "    \"\"\"\n",
    "    output_attn = output_ids.attentions\n",
    "    pref_len = output_attn[0][0].shape[3]\n",
    "    full_len = output_attn[-1][0].shape[3]\n",
    "    prefill_attn = output_attn[0]\n",
    "    assert prefill_attn[0].shape[0] == 1, \"batch size should be 1\"\n",
    "    full_attn = []\n",
    "\n",
    "    for l, layer in enumerate(prefill_attn):\n",
    "        layer = layer.cpu().squeeze(0).float()\n",
    "        layer = torch.nn.functional.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "        for i in range(full_len - pref_len):\n",
    "            cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "            layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "        full_attn.append(layer)\n",
    "    mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "    return mean_attn\n",
    "\n",
    "\n",
    "def get_visual_token_mean_attn_score(mean_attn, inputs, vision_start_token_id, vision_end_token_id) -> Tuple[torch.Tensor, ...]:\n",
    "    r\"\"\"\n",
    "    Get the attention weights of the visual tokens\n",
    "    Args:\n",
    "        mean_attn: the mean attention weights of the prefilling and full attention, shape: (L, L)\n",
    "        inputs: the inputs of the model\n",
    "    Returns:\n",
    "        visual_token_attn_weights: the tuple of the attention weights of the visual tokens, each element shape: (V, V)\n",
    "    \"\"\"\n",
    "    assert inputs[\"input_ids\"].shape[0] == 1, \"batch size should be 1\"\n",
    "    pref_len = len(inputs['input_ids'][0])\n",
    "    vision_start_token_indices = torch.where(\n",
    "        inputs[\"input_ids\"][0] == vision_start_token_id\n",
    "    )[0]\n",
    "    vision_end_token_indices = torch.where(\n",
    "        inputs[\"input_ids\"][0] == vision_end_token_id\n",
    "    )[0]\n",
    "    # assert len(vision_start_token_indices) == len(vision_end_token_indices), \"vision start and end token idx should be the same\"\n",
    "    # print(vision_start_token_indices)\n",
    "    # print(vision_end_token_indices)\n",
    "    # iterate over multiple images\n",
    "    visual_token_attn_weights = tuple(\n",
    "        torch.mean(mean_attn[pref_len:, s + 1 : e], dim=0)\n",
    "        for s, e in zip(\n",
    "            vision_start_token_indices, vision_end_token_indices, strict=True\n",
    "        )\n",
    "    )\n",
    "    return visual_token_attn_weights\n",
    "\n",
    "\n",
    "def get_visual_token_weight(\n",
    "    visual_token_attn_score,\n",
    "    threshold,\n",
    "    keep_weight,\n",
    "    weighting_type: Literal[\"linear\", \"exp\", \"uniform\"] | str = \"linear\",\n",
    "    lowest_weight=0.0,\n",
    "):\n",
    "    sorted_indices = torch.argsort(visual_token_attn_score, descending=True)\n",
    "    num_tokens_to_keep = int(len(visual_token_attn_score) * threshold)\n",
    "    weight_vision_token = torch.zeros_like(visual_token_attn_score, dtype=torch.float)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = keep_weight\n",
    "    if weighting_type == \"linear\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(\n",
    "            lowest_weight, 1.0, len(visual_token_attn_score) - num_tokens_to_keep\n",
    "        )\n",
    "    elif weighting_type == \"exp\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.exp(\n",
    "            torch.linspace(0, -3, len(sorted_indices) - num_tokens_to_keep)\n",
    "        )\n",
    "    elif weighting_type == \"uniform\":\n",
    "        weight_vision_token[sorted_indices[num_tokens_to_keep:]] = lowest_weight\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid weighting type: {weighting_type}\")\n",
    "    return weight_vision_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llava_onevision.modeling_llava_onevision import (\n",
    "    LlavaOnevisionCausalLMOutputWithPast,\n",
    ")\n",
    "\n",
    "\n",
    "def patch_forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    pixel_values: torch.FloatTensor = None,\n",
    "    image_sizes: Optional[torch.LongTensor] = None,\n",
    "    pixel_values_videos: torch.FloatTensor = None,\n",
    "    image_sizes_videos: Optional[torch.LongTensor] = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    vision_feature_layer: Optional[Union[int, List[int]]] = None,\n",
    "    vision_feature_select_strategy: Optional[str] = None,\n",
    "    vision_aspect_ratio: Optional[str] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "    **lm_kwargs,\n",
    ") -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        logits_to_keep (`int` or `torch.Tensor`, *optional*):\n",
    "            If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "            `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "            token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "            If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n",
    "            This is useful when using packed tensor format (single dimension for batch and sequence length).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        [`~LlavaOnevisionCausalLMOutputWithPast`] (if `return_dict=True`) or a `tuple`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from PIL import Image\n",
    "    >>> import requests\n",
    "    >>> import torch\n",
    "    >>> from transformers import LlavaOnevisionProcessor, LlavaOnevisionForConditionalGeneration\n",
    "\n",
    "    >>> model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=\"float16\", device_map=\"cuda:0\")\n",
    "    >>> processor = LlavaOnevisionProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n",
    "\n",
    "    >>> conversation = [\n",
    "    ...     {\n",
    "    ...       \"role\": \"user\",\n",
    "    ...       \"content\": [\n",
    "    ...           {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "    ...           {\"type\": \"image\"},\n",
    "    ...         ],\n",
    "    ...     },\n",
    "    ... ]\n",
    "    >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    >>> image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    >>> raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "    >>> inputs = processor(text=prompt, images=raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "    >>> output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "    >>> processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    \"user\\n\\nWhat is shown in this image?\\nassistant\\ncat\"\n",
    "    ```\"\"\"\n",
    "    output_attentions = (\n",
    "        output_attentions\n",
    "        if output_attentions is not None\n",
    "        else self.config.output_attentions\n",
    "    )\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states\n",
    "        if output_hidden_states is not None\n",
    "        else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = (\n",
    "        return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    )\n",
    "    vision_feature_layer = (\n",
    "        vision_feature_layer\n",
    "        if vision_feature_layer is not None\n",
    "        else self.config.vision_feature_layer\n",
    "    )\n",
    "    vision_feature_select_strategy = (\n",
    "        vision_feature_select_strategy\n",
    "        if vision_feature_select_strategy is not None\n",
    "        else self.config.vision_feature_select_strategy\n",
    "    )\n",
    "    vision_aspect_ratio = (\n",
    "        vision_aspect_ratio\n",
    "        if vision_aspect_ratio is not None\n",
    "        else self.config.vision_aspect_ratio\n",
    "    )\n",
    "\n",
    "    if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "        raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "    if (\n",
    "        pixel_values is not None or pixel_values_videos is not None\n",
    "    ) and inputs_embeds is not None:\n",
    "        raise ValueError(\n",
    "            \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n",
    "            \"and must specify either one\"\n",
    "        )\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "    # Images are processed with Anyres\n",
    "    if pixel_values is not None:\n",
    "        image_features = self.get_image_features(\n",
    "            pixel_values,\n",
    "            image_sizes,\n",
    "            vision_feature_layer=vision_feature_layer,\n",
    "            vision_feature_select_strategy=vision_feature_select_strategy,\n",
    "        )\n",
    "        image_features, feature_lens = self.pack_image_features(\n",
    "            image_features,\n",
    "            image_sizes,\n",
    "            image_newline=self.image_newline,\n",
    "            vision_aspect_ratio=vision_aspect_ratio,\n",
    "        )\n",
    "\n",
    "        special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n",
    "        special_image_mask = special_image_mask.expand_as(inputs_embeds).to(\n",
    "            inputs_embeds.device\n",
    "        )\n",
    "        if (\n",
    "            not is_torchdynamo_compiling()\n",
    "            and inputs_embeds[special_image_mask].numel() != image_features.numel()\n",
    "        ):\n",
    "            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n",
    "            n_image_features = image_features.shape[0]\n",
    "            raise ValueError(\n",
    "                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "            )\n",
    "        image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        if self.embed_weight is not None:\n",
    "            image_features *= self.embed_weight[:, None]\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n",
    "\n",
    "    # Video are simply embedded and further pooled to decrease seq len\n",
    "    if pixel_values_videos is not None:\n",
    "        video_features = self.get_video_features(\n",
    "            pixel_values_videos,\n",
    "            vision_feature_layer=vision_feature_layer,\n",
    "            vision_feature_select_strategy=vision_feature_select_strategy,\n",
    "        )\n",
    "        image_newline = (\n",
    "            self.image_newline[None, None, :]\n",
    "            .repeat(video_features.shape[0], 1, 1)\n",
    "            .to(video_features.device)\n",
    "        )\n",
    "        video_features = torch.cat((video_features, image_newline), dim=1)\n",
    "        video_features = video_features.flatten(0, 1)\n",
    "\n",
    "        special_video_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n",
    "        special_video_mask = special_video_mask.expand_as(inputs_embeds).to(\n",
    "            inputs_embeds.device\n",
    "        )\n",
    "        if (\n",
    "            not is_torchdynamo_compiling()\n",
    "            and inputs_embeds[special_image_mask].numel() != video_features.numel()\n",
    "        ):\n",
    "            n_video_tokens = (input_ids == self.config.video_token_index).sum()\n",
    "            n_video_features = video_features.shape[0]\n",
    "            raise ValueError(\n",
    "                f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n",
    "            )\n",
    "        video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n",
    "\n",
    "    outputs = self.language_model(\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        cache_position=cache_position,\n",
    "        logits_to_keep=logits_to_keep,\n",
    "        **lm_kwargs,\n",
    "    )\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        # Shift so that tokens < n predict n\n",
    "        if attention_mask is not None:\n",
    "            # we use the input attention mask to shift the logits and labels, because it is 2D.\n",
    "            # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n",
    "            shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(\n",
    "                logits.device\n",
    "            )\n",
    "            shift_logits = logits[..., :-1, :][\n",
    "                shift_attention_mask.to(logits.device) != 0\n",
    "            ].contiguous()\n",
    "            shift_labels = labels[..., 1:][\n",
    "                shift_attention_mask.to(labels.device) != 0\n",
    "            ].contiguous()\n",
    "        else:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1).to(shift_logits.device),\n",
    "        )\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits,) + outputs[1:]\n",
    "        return (loss,) + output if loss is not None else output\n",
    "\n",
    "    return LlavaOnevisionCausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        past_key_values=outputs.past_key_values,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "        image_hidden_states=image_features if pixel_values is not None else None,\n",
    "        video_hidden_states=video_features if pixel_values_videos is not None else None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
