{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "import transformers\n",
    "import transformers.modeling_attn_mask_utils\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForImageTextToText,\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaOnevisionForConditionalGeneration,\n",
    ")\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "from logging import config\n",
    "import warnings\n",
    "import cv2\n",
    "\n",
    "# Custom color map, from all transparent to all magenta\n",
    "# Create a custom colormap that goes from transparent to magenta\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "\n",
    "# Define the colors: transparent to Green\n",
    "# colors = [(0, 1, 0, 0), (0, 1, 0, 1)]  # RGBA format: (R, G, B, alpha)\n",
    "# cmap_name = 'transparent_to_green'\n",
    "# cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=256)\n",
    "colors = [(1, 0, 1, 0), (1, 0, 1, 1)]  # RGBA format: (R, G, B, alpha)\n",
    "cmap_name = \"transparent_to_magenta\"\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"llava-hf/llava-onevision-qwen2-0.5b-si-hf\"\n",
    "# model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "# model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    # low_cpu_mem_usage=True,\n",
    ").to(\"cuda:3\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "out_attns = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # module.config._attn_implementation_autoset = True\n",
    "        # module.config._attn_implementation = \"sdpa\"\n",
    "        # module.config._attn_implementation_internal = \"sdpa\"\n",
    "        # print(type(module))\n",
    "        # print(type(super(type(module), module)))\n",
    "        \n",
    "        print(f'{name}:\\n Type: {type(module)}, Attn: {module.config._attn_implementation}, Internal: {module.config._attn_implementation_internal}, Autosetted: {module.config._attn_implementation_autoset}')\n",
    "        \n",
    "        # print(module.config._attn_implementation_autoset)\n",
    "        # print(module.config._attn_implementation_internal)\n",
    "        # print(module.config._attn_implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS_TO_EXPORT_ATTN_MAP = [15]\n",
    "LAYERS_TO_EXPORT_ATTN_MAP = [16, 24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_attn_hooks(\n",
    "    attn: nn.Module,\n",
    "    attn_outputs: list[torch.Tensor],\n",
    "    *,\n",
    "    clear_hooks: bool = False,\n",
    "    clear_attn_output: bool = False,\n",
    ") -> tuple[RemovableHandle, RemovableHandle]:\n",
    "    # Clear Attention Output List if necessary\n",
    "    if clear_attn_output:\n",
    "        attn_outputs.clear()\n",
    "    # Patch the attention mask if using SDPA originally\n",
    "    # This is a hack to enforce the attention mask work with SDPA\n",
    "    if (\n",
    "        attn.config._attn_implementation == \"sdpa\"\n",
    "        or attn.config._attn_implementation_internal == \"sdpa\"\n",
    "    ):\n",
    "        AttentionMaskConverter._ignore_causal_mask_sdpa = lambda *_, **__: False\n",
    "    # Set attention implementation to eager if necessary\n",
    "    # if (\n",
    "    #     attn.config._attn_implementation_autoset\n",
    "    #     or attn.config._attn_implementation != \"eager\"\n",
    "    #     or attn.config._attn_implementation_internal != \"eager\"\n",
    "    # ):\n",
    "    attn.config = deepcopy(attn.config)\n",
    "    attn.config._attn_implementation = \"eager\"\n",
    "    attn.config._attn_implementation_internal = \"eager\"\n",
    "    attn.config._attn_implementation_autoset = False\n",
    "    # Patch the forward method to use the super class's forward method\n",
    "    # aka. the original eager forward method\n",
    "    attn.forward = super(type(attn), attn).forward\n",
    "\n",
    "    # Clear hooks if requested\n",
    "    if clear_hooks and (len(attn._forward_pre_hooks) or len(attn._forward_hooks)):\n",
    "        warnings.warn(\n",
    "            \"Clearing ALL forward hooks, this is not recommended as it also clears hooks like accelerator hooks\"\n",
    "        )\n",
    "        attn._forward_pre_hooks = OrderedDict()\n",
    "        attn._forward_hooks = OrderedDict()\n",
    "    # Create pre-forward hook, for modifying kwargs\n",
    "    pre_fwd_hook = attn.register_forward_pre_hook(\n",
    "        lambda module, args, kwargs: kwargs.update({\"output_attentions\": True}),\n",
    "        with_kwargs=True,\n",
    "    )\n",
    "    post_fwd_hook = attn.register_forward_hook(\n",
    "        lambda module, args, output: attn_outputs.append(output[1].cpu())\n",
    "    )\n",
    "    return pre_fwd_hook, post_fwd_hook\n",
    "\n",
    "\n",
    "def make_layer_attn_hooks(\n",
    "    model_list: nn.ModuleList,\n",
    "    layers: list[int],\n",
    "    attn_outputs: dict[int, list[torch.Tensor]],\n",
    "    *,\n",
    "    clear_hooks: bool = False,\n",
    "    clear_attn_output: bool = False,\n",
    ") -> list[tuple[RemovableHandle, RemovableHandle]]:\n",
    "    if clear_attn_output:\n",
    "        attn_outputs.clear()\n",
    "\n",
    "    fwd_hooks = []\n",
    "    for layer_idx in layers:\n",
    "        attn_outputs[layer_idx] = []\n",
    "        attn = model_list[layer_idx].self_attn\n",
    "\n",
    "        pre_fwd_hook, post_fwd_hook = make_attn_hooks(\n",
    "            attn,\n",
    "            attn_outputs[layer_idx],\n",
    "            clear_hooks=clear_hooks,\n",
    "            clear_attn_output=clear_attn_output,\n",
    "        )\n",
    "\n",
    "        fwd_hooks.append((pre_fwd_hook, post_fwd_hook))\n",
    "\n",
    "    return fwd_hooks\n",
    "\n",
    "\n",
    "def merge_decoding_attn_maps(attn_maps: list[torch.Tensor]) -> torch.Tensor:\n",
    "    # attn_maps[0]: prefilling proc. attn maps, (batch_size, n_heads, n_prompt_tokens, n_prompt_tokens)\n",
    "    # attn_maps[1:]: decoding proc. attn maps, (batch_size, n_heads, 1, current_token_index)\n",
    "    # merge the decoding proc. attn maps to the prefilling proc. attn maps\n",
    "    # return: (batch_size, n_heads, n_total_tokens, n_total_tokens)\n",
    "    # Check if there are any attention maps to merge\n",
    "    if not attn_maps:\n",
    "        return torch.empty(0)\n",
    "\n",
    "    # Get the prefill attention map (first element)\n",
    "    prefill_map = attn_maps[0]\n",
    "\n",
    "    # If there's only the prefill map, return it directly\n",
    "    if len(attn_maps) == 1:\n",
    "        return prefill_map\n",
    "\n",
    "    # Extract dimensions\n",
    "    batch_size, n_heads, n_prompt_tokens, _ = prefill_map.shape\n",
    "\n",
    "    # Initialize the output tensor with zeros only where needed to optimize memory usage\n",
    "    n_total_tokens = n_prompt_tokens + len(attn_maps) - 1\n",
    "    merged_attn = torch.zeros(\n",
    "        (batch_size, n_heads, n_total_tokens, n_total_tokens),\n",
    "        dtype=prefill_map.dtype,\n",
    "        device=prefill_map.device,\n",
    "        # Use sparse initialization to save memory for large attention maps\n",
    "        layout=prefill_map.layout,\n",
    "    )\n",
    "\n",
    "    # Use in-place operation for copying the prefill attention map to avoid creating temporary tensors\n",
    "    merged_attn[:, :, :n_prompt_tokens, :n_prompt_tokens].copy_(prefill_map)\n",
    "\n",
    "    # Pre-calculate indices to avoid redundant computations in the loop\n",
    "    prompt_end = n_prompt_tokens\n",
    "\n",
    "    # Process decoding attention maps in a single batch where possible\n",
    "    if len(attn_maps) > 1:\n",
    "        for i, attn_map in enumerate(attn_maps[1:], 1):\n",
    "            pos = prompt_end + i - 1\n",
    "            # Use narrow/slice operations instead of creating new tensors\n",
    "            merged_attn[:, :, pos, : pos + 1] = attn_map[:, :, 0, : pos + 1]\n",
    "\n",
    "    return merged_attn\n",
    "\n",
    "\n",
    "out_attns = {}\n",
    "attn_hooks = make_layer_attn_hooks(\n",
    "    model.model.layers,\n",
    "    LAYERS_TO_EXPORT_ATTN_MAP,\n",
    "    out_attns,\n",
    "    clear_attn_output=True,\n",
    "    clear_hooks=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list((h1.remove(), h2.remove()) for h1, h2 in attn_hooks)\n",
    "# # Register pre forward hook and post forward hook to export attention weights\n",
    "# def pre_forward_hook(module, args, kwargs):\n",
    "#     print(\"Pre forward hook\")\n",
    "#     print(len(args))\n",
    "#     print(kwargs.keys())\n",
    "\n",
    "# def get_attention_scores_post_forward_hook(module, args, kwargs, output):\n",
    "#     # print(\"Post forward hook\")\n",
    "#     # print(len(args))\n",
    "#     # print(kwargs.keys())\n",
    "\n",
    "\n",
    "# # Add hooks to the attention layer\n",
    "# pre_fwd_hook = model.language_model.model.layers[LAYER_TO_EXPORT_ATTN_MAP].self_attn.register_forward_pre_hook(pre_forward_hook, with_kwargs=True)\n",
    "# post_fwd_hook = model.language_model.model.layers[LAYER_TO_EXPORT_ATTN_MAP].self_attn.register_forward_hook(post_forward_hook, with_kwargs=True)\n",
    "\n",
    "# print(model.language_model.model.layers[LAYER_TO_EXPORT_ATTN_MAP].self_attn.config)\n",
    "# config = deepcopy(model.language_model.model.layers[LAYER_TO_EXPORT_ATTN_MAP].self_attn.config)\n",
    "# config._attn_implementation_autoset = False\n",
    "# config._attn_implementation = 'eager'\n",
    "# config._attn_implementation_internal = 'eager'\n",
    "# model.language_model.model.layers[LAYER_TO_EXPORT_ATTN_MAP].self_attn.config = config\n",
    "# print(model.language_model.model.layers[LAYER_TO_EXPORT_ATTN_MAP].self_attn.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.tokenizer.added_tokens_encoder)\n",
    "print(model.config)\n",
    "print(model.generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"path\": \"examples/os_core.png\"},\n",
    "            {\"type\": \"text\", \"text\": \"Briefly describe the image in few words.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "short_conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Please echo the following text: <|image_start|><|im_end|><|image_end|><|text_start|><|text_end|>\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "long_conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"path\": \"examples/image.png\"},\n",
    "            {\"type\": \"text\", \"text\": \"Extract all the information from the table, and explain it in detail.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "long_conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"path\": \"examples/img2.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Count the number of green objects in the image.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "## Old way of template & tokenization\n",
    "# prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "# raw_image = Image.open(\"examples/sample.png\")\n",
    "# inputs = processor(\n",
    "#     # images=raw_image,\n",
    "#     text=prompt,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(\n",
    "#     device=model.device,\n",
    "#     dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# All in one\n",
    "inputs = processor.apply_chat_template(\n",
    "    # short_conversation,\n",
    "    long_conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\n",
    "    device=model.device,\n",
    "    dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f'batch size: {inputs.input_ids.shape[0]}')\n",
    "    print(f'len of input prompt tokens: {inputs.input_ids.shape[1]}')\n",
    "    print(inputs.input_ids.shape)\n",
    "    print(inputs.input_ids.flatten().tolist())\n",
    "    print(processor.decode(inputs.input_ids[0]))\n",
    "    print(inputs.pixel_values.shape)\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in out_attns.values():\n",
    "    a.clear()\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    top_p=1.0,\n",
    "    # temperature=0.0,\n",
    "    # top_k=0,\n",
    "    # output_attentions=True,\n",
    "    # output_hidden_states=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "print(\n",
    "    processor.decode(\n",
    "        output[0] if isinstance(output, Tensor) else output[\"sequences\"][0],\n",
    "        # skip_special_tokens=True,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_attns = {i: merge_decoding_attn_maps(out_attn) for i, out_attn in out_attns.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    v_start, v_end = (\n",
    "        inputs.input_ids.flatten()\n",
    "        .tolist()\n",
    "        .index(processor.tokenizer.added_tokens_encoder[\"<|vision_start|>\"])\n",
    "        + 1,\n",
    "        inputs.input_ids.flatten()\n",
    "        .tolist()\n",
    "        .index(processor.tokenizer.added_tokens_encoder[\"<|vision_end|>\"]),\n",
    "    )\n",
    "    _, h, w = inputs.image_grid_thw[0] // 2\n",
    "except:\n",
    "    v_start, v_end = 0, 0\n",
    "    h, w = 0, 0\n",
    "else:\n",
    "    image = Image.open(long_conversation[0][\"content\"][0][\"path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len of output_ids: {output.shape[1] if isinstance(output, Tensor) else output[\"sequences\"].shape[1]}')\n",
    "print(f'len of out_attn: {len(out_attns[16])}')\n",
    "print(f'len of prefill: {out_attns[16][0].shape[-1]}')\n",
    "print(f'len of decoding: {out_attns[16][-1].shape[-1] + 1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(out_attns), figsize=(10, 10))\n",
    "\n",
    "for i, (layer_idx, out_attn) in enumerate(out_attns.items()):\n",
    "    ax = axes[i] if len(out_attns) > 1 else axes\n",
    "    im = ax.imshow(out_attn[0][0].mean(dim=0)[1:, 1:].float().numpy(), cmap=\"viridis\")\n",
    "    ax.axvspan(v_start - 1.5, v_end - 1.5, color=\"red\", alpha=0.1)\n",
    "    ax.set_title(f\"Layer {layer_idx}\")\n",
    "plt.suptitle(\"Prefill Attention Map\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(out_attns), figsize=(10, 10))\n",
    "\n",
    "for i, (layer_idx, merged_attn) in enumerate(merged_attns.items()):\n",
    "    # print(merged_attn.shape)\n",
    "    ax = axes[i] if len(merged_attns) > 1 else axes\n",
    "    im = ax.imshow(merged_attn[0].mean(dim=0)[1:, 1:].float().numpy(), cmap=\"viridis\")\n",
    "    ax.axhspan(-0.5, inputs.input_ids.shape[1] - 1.5, color=\"blue\", alpha=0.1)\n",
    "    ax.axvspan(v_start - 1.5, v_end - 1.5, color=\"red\", alpha=0.1)\n",
    "    ax.axhline(inputs.input_ids.shape[1] - 1.5, color=\"red\", linestyle=\":\")\n",
    "    ax.axvline(inputs.input_ids.shape[1] - 1.5, color=\"red\", linestyle=\":\")\n",
    "    ax.set_title(f\"Layer {layer_idx}\")\n",
    "    # plt.colorbar(im, ax=ax)\n",
    "plt.suptitle(\"Full Attention Map\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of tokens to visualize\n",
    "num_tokens = output[\"sequences\"].shape[1] - len(inputs.input_ids.flatten())\n",
    "num_layers = len(out_attns.keys())\n",
    "total_figures = num_tokens * num_layers\n",
    "\n",
    "# Create a reasonable grid layout\n",
    "rows = total_figures // 8\n",
    "cols = 8\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=rows, ncols=cols)\n",
    "# if rows == 1 and cols == 1:\n",
    "#     axes = np.array([axes])  # Handle single subplot case\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# Initialize plot index\n",
    "i = 0\n",
    "token_offset = len(inputs.input_ids.flatten())\n",
    "\n",
    "# Loop through each generated token\n",
    "for token_index in range(num_tokens):\n",
    "    # Get the actual token index in the sequence\n",
    "    seq_token_idx = token_index + token_offset\n",
    "\n",
    "    # Print the token being visualized\n",
    "    if seq_token_idx < len(output[\"sequences\"][0]):\n",
    "        token_text = processor.decode(output[\"sequences\"][0][seq_token_idx])\n",
    "        print(f\"Token {token_index}: {token_text}\")\n",
    "\n",
    "    # Visualize attention for each layer\n",
    "    for layer_idx, attn in merged_attns.items():\n",
    "        try:\n",
    "            # Create attention heatmap\n",
    "            attention_map = (\n",
    "                attn[0]\n",
    "                .mean(dim=0)\n",
    "                [seq_token_idx - 1, v_start:v_end]\n",
    "                .reshape(h, w)\n",
    "                .cpu()\n",
    "                .float()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "            # Calculate the entropy of the attention map\n",
    "            normalized_attention_map = (attention_map / np.sum(attention_map)).ravel()\n",
    "            entropy = -np.sum(normalized_attention_map * np.log(normalized_attention_map + 1e-10))\n",
    "            print(f\"Entropy of layer {layer_idx}: {entropy}\")\n",
    "\n",
    "            # Upsample the attention map to match the image size\n",
    "            attention_map = cv2.resize(\n",
    "                attention_map,\n",
    "                (image.width, image.height),\n",
    "                interpolation=cv2.INTER_NEAREST,\n",
    "            )\n",
    "\n",
    "            # Plot the image\n",
    "            plt.imshow(image)\n",
    "\n",
    "            # Overlay the attention map with transparency\n",
    "            plt.imshow(attention_map, cmap=cm)\n",
    "            \n",
    "            # Disable the axis\n",
    "            # plt.axis(\"off\")\n",
    "            plt.title(f\"Layer {layer_idx}, Token {token_index}: {token_text}\")\n",
    "            plt.savefig(f\"attn_maps/attention_map_{layer_idx}_{token_index}.png\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        i += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "# for j in range(i, len(axes)):\n",
    "#     axes[j].axis(\"off\")\n",
    "\n",
    "# plt.savefig(\"attention_maps.svg\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[out_attn.__len__() for out_attn in out_attns.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_attns[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_start, v_end = inputs.input_ids.flatten().tolist().index(processor.tokenizer.added_tokens_encoder[\"<|vision_start|>\"])+1, inputs.input_ids.flatten().tolist().index(processor.tokenizer.added_tokens_encoder[\"<|vision_end|>\"])\n",
    "_, h, w = inputs.image_grid_thw[0] // 2\n",
    "\n",
    "for attn_high, attn_low in zip(out_attns[5], out_attns[14], strict=True):\n",
    "  # Visualize the attention maps\n",
    "  \n",
    "  # Visualize attn_high\n",
    "  # Create a figure with two subplots side by side\n",
    "  fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "  \n",
    "  # Plot high layer attention map\n",
    "  im1 = axes[0, 0].imshow(attn_high.mean(dim=(0,1))[-1, v_start:v_end].reshape(h, w).float().numpy(), cmap='viridis')\n",
    "  axes[0, 0].set_title('High Layer Attention Map')\n",
    "  axes[0, 0].set_xlabel('Token Index (Target)')\n",
    "  axes[0, 0].set_ylabel('Token Index (Source)')\n",
    "  fig.colorbar(im1, ax=axes[0, 0], label='Attention Weight')\n",
    "  im1.set_clim(0, 0.05)\n",
    "  \n",
    "  # Plot low layer attention map\n",
    "  im2 = axes[0, 1].imshow(attn_low.mean(dim=(0,1))[-1, v_start:v_end].reshape(h, w).float().numpy(), cmap='viridis')\n",
    "  axes[0, 1].set_title('Low Layer Attention Map')\n",
    "  axes[0, 1].set_xlabel('Token Index (Target)')\n",
    "  axes[0, 1].set_ylabel('Token Index (Source)')\n",
    "  fig.colorbar(im2, ax=axes[0, 1], label='Attention Weight')\n",
    "  im2.set_clim(0, 0.05)\n",
    "\n",
    "  # Plot difference between high and low layer attention maps\n",
    "  im3 = axes[1, 0].imshow((attn_high - attn_low).mean(dim=(0,1)).exp().float().numpy()[:, 1:], cmap='viridis')\n",
    "  axes[1, 0].set_title('Difference between High and Low Layer Attention Maps')\n",
    "  axes[1, 0].set_xlabel('Token Index (Target)')\n",
    "  axes[1, 0].set_ylabel('Token Index (Source)')\n",
    "  fig.colorbar(im3, ax=axes[1, 0], label='Attention Weight')\n",
    "  \n",
    "  \n",
    "  plt.tight_layout()\n",
    "  # plt.figure(figsize=(10, 8))\n",
    "  # print((attn_high - attn_low).mean(dim=(0,1)).float().numpy())\n",
    "  # plt.imshow(attn_high.mean(dim=(0,1)).float().numpy()[:, 1:], cmap='viridis')\n",
    "  # plt.colorbar(label='Attention Weight')\n",
    "  # plt.title('High Layer Attention Map')\n",
    "  # plt.xlabel('Token Index (Target)')\n",
    "  # plt.ylabel('Token Index (Source)')\n",
    "  # # Set the colorbar range from 0 to 0.3\n",
    "  # plt.clim(0, 0.05)\n",
    "  # plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_end - v_start - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.added_tokens_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
