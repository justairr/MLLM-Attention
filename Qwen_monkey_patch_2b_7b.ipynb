{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"MAX_PIXELS\"]=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f90eebb9994e5d8153b4d23096a79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69601882962b4511abdb0636209da5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_7b = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    #  attn_implementation=\"sdpa\",\n",
    "    #  device_map=\"auto\",\n",
    ").to(\"cuda:2\")\n",
    "model_2b = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    # device_map=\"auto\",\n",
    ").to(\"cuda:3\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "# processor_2b = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-18 11:37:45] ERROR - misc.py: load_env - 212: Did not detect the .env file at /home/seas0/SyncedWorkspace/MLLM-Attention/VLMEvalKit/.env, failed to load. \n",
      "[2025-03-18 11:37:45] ERROR - misc.py: load_env - 212: Did not detect the .env file at /home/seas0/SyncedWorkspace/MLLM-Attention/VLMEvalKit/.env, failed to load. \n"
     ]
    }
   ],
   "source": [
    "from VLMEvalKit.vlmeval.vlm.qwen2_vl.qwen_mod_forward import (\n",
    "    patch_forward,\n",
    "    get_mean_attn_score,\n",
    "    get_visual_token_mean_attn_score,\n",
    "    get_visual_token_weight,\n",
    "    calculate_dynamic_threshold,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are a helpful assistant. Here is one image:\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/image.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Here is another image:\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/sample.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "#print(image_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_images(image_list, mode=\"noise\", color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    根据 `mode` 生成与 `image_list` 中图像等大的随机噪声或纯色图像。\n",
    "\n",
    "    参数：\n",
    "    - image_list: list[PIL.Image]，输入的图像列表。\n",
    "    - mode: str，\"noise\" 生成随机噪声图像，\"blank\" 生成纯色图像。\n",
    "    - color: tuple，生成纯色图像时的颜色，默认为白色 (255, 255, 255)。\n",
    "\n",
    "    返回：\n",
    "    - list[PIL.Image]，生成的图像列表。\n",
    "    \"\"\"\n",
    "    generated_images = []\n",
    "    \n",
    "    for img in image_list:\n",
    "        width, height = img.size\n",
    "        \n",
    "        if mode == \"noise\":\n",
    "            noise_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n",
    "            generated_image = Image.fromarray(noise_array)\n",
    "        elif mode == \"blank\":\n",
    "            generated_image = Image.new(\"RGB\", (width, height), color)\n",
    "        else:\n",
    "            raise ValueError(\"mode could only be 'noise' or 'blank'\")\n",
    "        \n",
    "        generated_images.append(generated_image)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "neg_image_inputs = generate_images(image_inputs, mode=\"noise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda:3\")\n",
    "output_ids = model_2b.generate(\n",
    "            **inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3032,  1.5508,  1.4632,  ..., -0.3000,  2.0321, -1.4376],\n",
      "        [ 1.5800, -0.1134,  1.1712,  ...,  1.7762, -1.3949,  0.0840],\n",
      "        [ 0.0763,  0.1055, -1.7485,  ..., -0.4279,  1.4349,  0.0982],\n",
      "        ...,\n",
      "        [ 0.0179,  1.6676,  0.1639,  ...,  0.5248, -0.7266, -1.3807],\n",
      "        [ 0.4851, -1.7339,  0.5873,  ...,  1.2500, -0.9399,  2.1459],\n",
      "        [ 1.9011,  0.4121,  0.5727,  ...,  1.5344,  1.1789,  0.1977]],\n",
      "       device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "neg_inputs = processor(\n",
    "    text=[text],\n",
    "    images=neg_image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "neg_inputs = neg_inputs.to(\"cuda:3\")\n",
    "print(neg_inputs.pixel_values)\n",
    "negative_output_ids = model_2b.generate(\n",
    "            **neg_inputs,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to be a highly pixelated and distorted representation of a scene. The colors are vibrant and the pattern is chaotic, making it difficult to discern any specific details or objects within the image. The overall effect is one of visual noise and disorientation.\n"
     ]
    }
   ],
   "source": [
    "# If you wanna check what's the output of negative generation, run me.\n",
    "\n",
    "generated_ids = negative_output_ids.sequences\n",
    "# generated_ids = output_ids.sequences\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1276, 1276])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aw = get_mean_attn_score(output_ids)\n",
    "\n",
    "# neg_aw = get_mean_attn_score(negative_output_ids)\n",
    "aw.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([440])\n",
      "torch.Size([666])\n"
     ]
    }
   ],
   "source": [
    "vw = get_visual_token_mean_attn_score(\n",
    "    aw,\n",
    "    inputs,\n",
    "    model_2b.config.vision_start_token_id,\n",
    "    model_2b.config.vision_end_token_id,\n",
    ")\n",
    "\n",
    "# neg_vw = get_visual_token_mean_attn_score(\n",
    "#     neg_aw,\n",
    "#     neg_inputs,\n",
    "#     model_2b.config.vision_start_token_id,\n",
    "#     model_2b.config.vision_end_token_id,\n",
    "# )\n",
    "_ = [print(v.shape) for v in vw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_linear = [\n",
    "  get_visual_token_weight(v, 0.6, 1.0, \"linear\", 0.0) for v in vw\n",
    "]\n",
    "model_7b.embed_weight = torch.concat(vm_linear, dim=0).to(model_7b.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7b.forward = patch_forward.__get__(model_7b, Qwen2VLForConditionalGeneration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model_7b.generate(**inputs.to(model_7b.device), max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and echo output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids.sequences)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from torch import Tensor\n",
    "import copy\n",
    "\n",
    "vision_start_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_start_token_id)\n",
    "vision_end_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_end_token_id)\n",
    "\n",
    "output_attn: Tuple[Tuple[Tensor, ...], ...] = copy.deepcopy(output_ids.attentions)\n",
    "# get the length of the prefilling and full attention\n",
    "pref_len: int = output_attn[0][0].shape[3]\n",
    "full_len: int = output_attn[-1][0].shape[3]\n",
    "prefill_attn: Tuple[Tensor, ...] = output_attn[0]\n",
    "\n",
    "# batchsize should be 1\n",
    "assert prefill_attn[0].shape[0] == 1\n",
    "full_attn = []\n",
    "for l, layer in enumerate(prefill_attn):\n",
    "    layer = layer.cpu().squeeze(0).float()\n",
    "    layer = F.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "    for i in range(full_len - pref_len):\n",
    "        # print(i, )\n",
    "        # cur_attn = output_attn[i][l].cpu().squeeze(0).float()\n",
    "        cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "        # print(cur_attn.shape)\n",
    "        layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "    full_attn.append(layer)\n",
    "mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "\n",
    "image_output_attn = torch.mean(mean_attn[pref_len:, vision_start_token_idx + 1:vision_end_token_idx], dim=0)\n",
    "\n",
    "def calculate_dynamic_threshold(attn, percentile=98):\n",
    "    hist = torch.histc(attn, bins=100)\n",
    "    cdf = torch.cumsum(hist, dim=0)/torch.sum(hist)\n",
    "    threshold = torch.argmax((cdf > percentile/100).float()).item()/100\n",
    "    return threshold\n",
    "\n",
    "threshold = calculate_dynamic_threshold(image_output_attn)\n",
    "print(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighted_vision_tokens(attn_map, keep_percentage=threshold):\n",
    "    # Get the attention values sorted in descending order\n",
    "    sorted_attention, sorted_indices = torch.sort(attn_map, descending=True)\n",
    "    \n",
    "    # Determine the number of tokens to keep\n",
    "    num_tokens_to_keep = int(len(sorted_attention) * keep_percentage)\n",
    "    \n",
    "    # Create a weight mask where the top tokens have higher weight\n",
    "    weight_vision_token = torch.zeros_like(attn_map, dtype=torch.float)\n",
    "    \n",
    "    # Assign weights for tokens (top tokens get higher weights, others get smaller weights)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(0.6, 1.0, len(sorted_attention) - num_tokens_to_keep)\n",
    "\n",
    "    return weight_vision_token\n",
    "    \n",
    "weight_vision_token = reweighted_vision_tokens(image_output_attn).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vision_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_vision_token.size()\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "print(n_image_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "\n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.type(model.visual.get_dtype())\n",
    "    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "    n_image_features = image_embeds.shape[0]\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(\n",
    "            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        )\n",
    "    image_mask = (\n",
    "        (input_ids == model.config.image_token_id)\n",
    "        .unsqueeze(-1)\n",
    "        .expand_as(inputs_embeds)\n",
    "        .to(inputs_embeds.device)\n",
    "    )\n",
    "    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    print(image_embeds)\n",
    "    image_embeds *= weight_vision_token[:, None]\n",
    "    print(image_embeds)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "print(image_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
