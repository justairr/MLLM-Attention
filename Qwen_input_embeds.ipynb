{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"MAX_PIXELS\"]="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    " \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    " torch_dtype=torch.bfloat16,\n",
    " attn_implementation=\"eager\",\n",
    " device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"examples/image.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "        \n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.type(model.visual.get_dtype())\n",
    "    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "    n_image_features = image_embeds.shape[0]\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\")\n",
    "    \n",
    "    image_mask = (input_ids == model.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n",
    "    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "generated_ids = model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, max_new_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "out = processor.tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "response = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and echo output\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids.sequences)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from torch import Tensor\n",
    "import copy\n",
    "\n",
    "vision_start_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_start_token_id)\n",
    "vision_end_token_idx = inputs['input_ids'][0].tolist().index(model.config.vision_end_token_id)\n",
    "\n",
    "output_attn: Tuple[Tuple[Tensor, ...], ...] = copy.deepcopy(output_ids.attentions)\n",
    "# get the length of the prefilling and full attention\n",
    "pref_len: int = output_attn[0][0].shape[3]\n",
    "full_len: int = output_attn[-1][0].shape[3]\n",
    "prefill_attn: Tuple[Tensor, ...] = output_attn[0]\n",
    "\n",
    "# batchsize should be 1\n",
    "assert prefill_attn[0].shape[0] == 1\n",
    "full_attn = []\n",
    "for l, layer in enumerate(prefill_attn):\n",
    "    layer = layer.cpu().squeeze(0).float()\n",
    "    layer = F.pad(layer, (0, full_len - pref_len, 0, full_len - pref_len))\n",
    "    for i in range(full_len - pref_len):\n",
    "        # print(i, )\n",
    "        # cur_attn = output_attn[i][l].cpu().squeeze(0).float()\n",
    "        cur_attn = output_attn[i + 1][l].cpu().squeeze(0)[:, 0, :].float()\n",
    "        # print(cur_attn.shape)\n",
    "        layer[:, pref_len + i, :pref_len + i + 1] = cur_attn\n",
    "    full_attn.append(layer)\n",
    "mean_attn = torch.stack(full_attn).mean(dim=(0, 1))\n",
    "\n",
    "image_output_attn = torch.mean(mean_attn[pref_len:, vision_start_token_idx + 1:vision_end_token_idx], dim=0)\n",
    "\n",
    "def calculate_dynamic_threshold(attn, percentile=98):\n",
    "    hist = torch.histc(attn, bins=100)\n",
    "    cdf = torch.cumsum(hist, dim=0)/torch.sum(hist)\n",
    "    threshold = torch.argmax((cdf > percentile/100).float()).item()/100\n",
    "    return threshold\n",
    "\n",
    "threshold = calculate_dynamic_threshold(image_output_attn)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_vision_attention(attn_map, keep_percentage=threshold):\n",
    "    # Get the attention values sorted in descending order\n",
    "    sorted_attention, sorted_indices = torch.sort(attn_map, descending=True)\n",
    "    \n",
    "    # Determine the number of tokens to keep\n",
    "    num_tokens_to_keep = int(len(sorted_attention) * keep_percentage)\n",
    "    \n",
    "    # Create a weight mask where the top tokens have higher weight\n",
    "    weight_vision_token = torch.zeros_like(attn_map, dtype=torch.float)\n",
    "    \n",
    "    # Assign weights for tokens (top tokens get higher weights, others get smaller weights)\n",
    "    weight_vision_token[sorted_indices[:num_tokens_to_keep]] = 1.0\n",
    "    weight_vision_token[sorted_indices[num_tokens_to_keep:]] = torch.linspace(0.6, 1.0, len(sorted_attention) - num_tokens_to_keep)\n",
    "\n",
    "    return weight_vision_token\n",
    "    \n",
    "weight_vision_token = weighted_vision_attention(image_output_attn).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vision_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_vision_token.size()\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "print(n_image_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "image_grid_thw = inputs[\"image_grid_thw\"]\n",
    "\n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.type(model.visual.get_dtype())\n",
    "    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\n",
    "    n_image_features = image_embeds.shape[0]\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(\n",
    "            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        )\n",
    "    image_mask = (\n",
    "        (input_ids == model.config.image_token_id)\n",
    "        .unsqueeze(-1)\n",
    "        .expand_as(inputs_embeds)\n",
    "        .to(inputs_embeds.device)\n",
    "    )\n",
    "    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    print(image_embeds)\n",
    "    image_embeds *= weight_vision_token[:, None]\n",
    "    print(image_embeds)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "print(image_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "_ = [print(output) for output in output_text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmabsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
